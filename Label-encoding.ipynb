{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "pd.options.display.max_columns = 30\n",
    "import pandas as pd\n",
    "import seaborn as sns # used for plot interactive graph. \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "883"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data = pd.read_csv(\"2_training_testing_80/New_training_data_19Jan2021_r.csv\",sep=',',header=0)\n",
    "testing_data = pd.read_csv(\"2_training_testing_80/new_Test_data_19Jan.csv\",sep=',',header=0)\n",
    "len(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Gender  Race  Ethnic Group    Encounter       Reason1       Reason2  \\\n",
      "0         1   0.0             1     57304086 -7.820000e-16  2.050000e-09   \n",
      "1         0   0.0             0     59464001 -8.640000e-18  1.160000e-10   \n",
      "2         1   1.0             0    306885119 -3.340000e-16  2.760000e-09   \n",
      "3         1   1.0             0    843895334 -1.130000e-16  1.300000e-09   \n",
      "4         0   1.0             0    964184337  5.560000e-17 -1.190000e-09   \n",
      "..      ...   ...           ...          ...           ...           ...   \n",
      "878       0   1.0             2  43280473142 -1.010000e-17 -4.370000e-12   \n",
      "879       0   0.0             0  43600834323  1.000000e+00  1.450000e-15   \n",
      "880       0   1.0             0  44604273292  3.800000e-17 -4.150000e-12   \n",
      "881       0   0.0             0  46328314129  7.280000e-17 -1.850000e-11   \n",
      "882       1   0.0             0  46642834260  1.000000e+00  1.450000e-15   \n",
      "\n",
      "          Reason3       Reason4       Reason5       Reason6       Reason7  \\\n",
      "0    4.910000e-03  1.800000e-02  2.390000e-03  4.740000e-01  3.640000e-02   \n",
      "1   -1.240000e-07 -5.670000e-07  8.990000e-07  6.690000e-07 -2.240000e-06   \n",
      "2    1.650000e-02  1.400000e-01  1.860000e-01 -1.910000e-02  1.300000e-01   \n",
      "3    6.560000e-01 -2.150000e-01  9.140000e-02 -5.680000e-03  5.800000e-02   \n",
      "4    9.900000e-03  3.780000e-01  8.710000e-01 -1.700000e-02 -1.340000e-01   \n",
      "..            ...           ...           ...           ...           ...   \n",
      "878  3.390000e-04  4.030000e-04 -1.010000e-05 -2.680000e-04  9.030000e-05   \n",
      "879  5.850000e-20 -1.390000e-19 -1.670000e-19 -1.300000e-18  1.960000e-18   \n",
      "880 -2.130000e-06 -6.860000e-06  2.100000e-05  1.440000e-06 -6.450000e-05   \n",
      "881  5.910000e-04  6.420000e-04  7.590000e-05 -4.400000e-04 -7.040000e-05   \n",
      "882  5.850000e-20 -1.390000e-19 -1.670000e-19 -1.300000e-18  1.960000e-18   \n",
      "\n",
      "          Reason8       Reason9      Reason10      Reason11      Reason12  \\\n",
      "0   -8.830000e-02  2.230000e-03 -1.370000e-03  2.210000e-03  5.500000e-02   \n",
      "1    5.130000e-06 -3.720000e-06 -6.840000e-06  1.940000e-05 -3.300000e-05   \n",
      "2    9.920000e-05  3.150000e-01 -7.670000e-02  4.840000e-03 -6.970000e-04   \n",
      "3    9.900000e-04 -2.780000e-02  3.020000e-02 -3.840000e-03  6.220000e-03   \n",
      "4   -1.610000e-03 -1.040000e-01  2.470000e-02 -2.000000e-03  2.630000e-03   \n",
      "..            ...           ...           ...           ...           ...   \n",
      "878 -3.610000e-02 -4.090000e-04  6.370000e-01  7.030000e-01  7.360000e-04   \n",
      "879 -6.580000e-19  1.220000e-19  4.470000e-19  7.540000e-19 -7.720000e-19   \n",
      "880  1.430000e-04  3.230000e-04 -8.030000e-05  2.210000e-04 -3.010000e-04   \n",
      "881  1.860000e-01  9.290000e-04  2.920000e-02 -1.860000e-02 -4.630000e-03   \n",
      "882 -6.580000e-19  1.220000e-19  4.470000e-19  7.540000e-19 -7.720000e-19   \n",
      "\n",
      "         Reason13      Reason14      Reason15      Reason16      Reason17  \\\n",
      "0   -1.620000e-01  6.550000e-02 -3.040000e-01 -3.040000e-03  9.610000e-03   \n",
      "1   -1.260000e-04 -7.580000e-05 -3.480000e-06 -6.080000e-06 -9.760000e-06   \n",
      "2    4.960000e-03  5.310000e-03 -8.430000e-02  3.640000e-02  6.160000e-02   \n",
      "3    3.230000e-02 -9.870000e-03 -3.440000e-02  5.090000e-01  8.190000e-02   \n",
      "4   -1.500000e-02  3.190000e-03 -5.350000e-03 -8.180000e-03 -1.310000e-02   \n",
      "..            ...           ...           ...           ...           ...   \n",
      "878  6.990000e-04  2.890000e-04 -7.910000e-04 -8.640000e-03 -5.840000e-03   \n",
      "879  3.700000e-19  4.260000e-19 -1.860000e-18  7.620000e-19  1.100000e-18   \n",
      "880 -4.780000e-05  1.750000e-04  6.800000e-04 -4.990000e-04 -5.740000e-03   \n",
      "881 -5.450000e-03  1.480000e-02  8.990000e-03  7.230000e-02 -8.380000e-02   \n",
      "882  3.700000e-19  4.260000e-19 -1.860000e-18  7.620000e-19  1.100000e-18   \n",
      "\n",
      "         Reason18      Reason19      Reason20  Class  Age Age_bins  \n",
      "0   -4.820000e-02 -6.180000e-02 -3.360000e-02      0   59        3  \n",
      "1    3.080000e-04 -5.720000e-05  7.290000e-05      0   75        3  \n",
      "2   -1.740000e-01  7.970000e-01 -6.290000e-02      0   79        3  \n",
      "3    4.940000e-02 -1.330000e-02  6.670000e-02      0   45        2  \n",
      "4    1.700000e-03 -1.440000e-01  3.190000e-02      0   55        2  \n",
      "..            ...           ...           ...    ...  ...      ...  \n",
      "878 -1.540000e-03 -5.910000e-03  2.620000e-02      1   65        3  \n",
      "879  1.460000e-18  2.510000e-18  8.600000e-20      1   33        1  \n",
      "880  4.830000e-04  1.110000e-03  8.830000e-05      1   23        1  \n",
      "881 -3.640000e-02 -3.140000e-01  1.480000e-01      1   82        3  \n",
      "882  1.460000e-18  2.510000e-18  8.600000e-20      1   62        3  \n",
      "\n",
      "[883 rows x 27 columns]\n"
     ]
    }
   ],
   "source": [
    "# For training Data\n",
    "bins = [0,15,35,55,96]\n",
    "labels=[0,1,2,3]\n",
    "training_data['Age_bins'] = pd.cut(training_data['Age'], bins=bins, labels=labels, include_lowest=True)\n",
    "print (training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Gender  Race  Ethnic Group    Encounter       Reason1       Reason2  \\\n",
      "0        1     1             2    892774098  1.000000e+00  5.660000e-16   \n",
      "1        0     1             2   1930374269  1.000000e+00  5.660000e-16   \n",
      "2        0     0             0   4090293179  1.000000e+00  1.450000e-15   \n",
      "3        0     0             2   5394323197  1.000000e+00  1.450000e-15   \n",
      "4        1     1             0   5903325002  1.000000e+00  1.450000e-15   \n",
      "..     ...   ...           ...          ...           ...           ...   \n",
      "75       1     6             2  50979275303  1.000000e+00  1.450000e-15   \n",
      "76       1     0             0  50998165297  1.000000e+00  5.660000e-16   \n",
      "77       1     0             0  51043285325  1.450000e-16 -3.320000e-10   \n",
      "78       1     0             0  51360305347  1.000000e+00  1.450000e-15   \n",
      "79       0     0             0  51362525349  1.000000e+00  1.450000e-15   \n",
      "\n",
      "         Reason3       Reason4       Reason5       Reason6       Reason7  \\\n",
      "0   2.060000e-16 -9.200000e-17  6.710000e-16 -7.300000e-16 -7.900000e-18   \n",
      "1   3.860000e-17  7.920000e-17 -1.750000e-17  2.080000e-16  1.060000e-17   \n",
      "2   5.850000e-20 -1.390000e-19 -1.670000e-19 -1.300000e-18  1.960000e-18   \n",
      "3   5.850000e-20 -1.390000e-19 -1.670000e-19 -1.300000e-18  1.960000e-18   \n",
      "4   5.850000e-20 -1.390000e-19 -1.670000e-19 -1.300000e-18  1.960000e-18   \n",
      "..           ...           ...           ...           ...           ...   \n",
      "75  5.850000e-20 -1.390000e-19 -1.670000e-19 -1.300000e-18  1.960000e-18   \n",
      "76 -1.630000e-18  4.140000e-20 -2.910000e-18  1.870000e-18 -1.850000e-19   \n",
      "77  7.760000e-02  4.400000e-01 -1.010000e-01 -8.130000e-02  8.650000e-01   \n",
      "78  5.850000e-20 -1.390000e-19 -1.670000e-19 -1.300000e-18  1.960000e-18   \n",
      "79  5.850000e-20 -1.390000e-19 -1.670000e-19 -1.300000e-18  1.960000e-18   \n",
      "\n",
      "         Reason8       Reason9      Reason10      Reason11      Reason12  \\\n",
      "0   9.820000e-16 -3.330000e-16  6.620000e-16 -1.090000e-16  9.510000e-17   \n",
      "1   5.940000e-16 -5.550000e-16  4.400000e-16  1.090000e-16 -3.460000e-16   \n",
      "2  -6.580000e-19  1.220000e-19  4.470000e-19  7.540000e-19 -7.720000e-19   \n",
      "3  -6.580000e-19  1.220000e-19  4.470000e-19  7.540000e-19 -7.720000e-19   \n",
      "4  -6.580000e-19  1.220000e-19  4.470000e-19  7.540000e-19 -7.720000e-19   \n",
      "..           ...           ...           ...           ...           ...   \n",
      "75 -6.580000e-19  1.220000e-19  4.470000e-19  7.540000e-19 -7.720000e-19   \n",
      "76 -5.250000e-18  2.570000e-18 -2.600000e-18 -1.500000e-18 -9.150000e-19   \n",
      "77  3.120000e-03 -8.510000e-02  6.850000e-02 -7.170000e-04  6.080000e-03   \n",
      "78 -6.580000e-19  1.220000e-19  4.470000e-19  7.540000e-19 -7.720000e-19   \n",
      "79 -6.580000e-19  1.220000e-19  4.470000e-19  7.540000e-19 -7.720000e-19   \n",
      "\n",
      "        Reason13      Reason14      Reason15      Reason16      Reason17  \\\n",
      "0   3.920000e-16  3.120000e-16  3.650000e-16 -3.330000e-16 -1.950000e-16   \n",
      "1  -1.880000e-16  1.990000e-16 -5.260000e-16 -9.010000e-17 -1.570000e-16   \n",
      "2   3.700000e-19  4.260000e-19 -1.860000e-18  7.620000e-19  1.100000e-18   \n",
      "3   3.700000e-19  4.260000e-19 -1.860000e-18  7.620000e-19  1.100000e-18   \n",
      "4   3.700000e-19  4.260000e-19 -1.860000e-18  7.620000e-19  1.100000e-18   \n",
      "..           ...           ...           ...           ...           ...   \n",
      "75  3.700000e-19  4.260000e-19 -1.860000e-18  7.620000e-19  1.100000e-18   \n",
      "76 -3.930000e-18  1.850000e-18  3.860000e-18  1.220000e-18 -1.240000e-18   \n",
      "77  2.270000e-02 -8.570000e-03  6.980000e-03 -7.210000e-03 -9.610000e-03   \n",
      "78  3.700000e-19  4.260000e-19 -1.860000e-18  7.620000e-19  1.100000e-18   \n",
      "79  3.700000e-19  4.260000e-19 -1.860000e-18  7.620000e-19  1.100000e-18   \n",
      "\n",
      "        Reason18      Reason19      Reason20  Class  Age Age_bins  \n",
      "0   1.270000e-16 -2.100000e-16 -8.340000e-17      0   61        3  \n",
      "1   3.340000e-16  2.190000e-16  8.180000e-17      0   77        3  \n",
      "2   1.460000e-18  2.510000e-18  8.600000e-20      1   61        3  \n",
      "3   1.460000e-18  2.510000e-18  8.600000e-20      1   76        3  \n",
      "4   1.460000e-18  2.510000e-18  8.600000e-20      1   53        2  \n",
      "..           ...           ...           ...    ...  ...      ...  \n",
      "75  1.460000e-18  2.510000e-18  8.600000e-20      1   71        3  \n",
      "76 -1.650000e-18  5.100000e-18 -6.780000e-18      0   64        3  \n",
      "77  2.520000e-02 -1.270000e-01  3.270000e-02      0   87        3  \n",
      "78  1.460000e-18  2.510000e-18  8.600000e-20      1   65        3  \n",
      "79  1.460000e-18  2.510000e-18  8.600000e-20      1   55        2  \n",
      "\n",
      "[80 rows x 27 columns]\n"
     ]
    }
   ],
   "source": [
    "# For Testing Data\n",
    "bins = [0,15,35,55,96]\n",
    "labels=[0,1,2,3]\n",
    "testing_data['Age_bins'] = pd.cut(testing_data['Age'], bins=bins, labels=labels, include_lowest=True)\n",
    "print (testing_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_dataset(df):\n",
    "    assert isinstance(df, pd.DataFrame), \"df needs to be a pd.DataFrame\"\n",
    "    df.dropna(inplace=True)\n",
    "    indices_to_keep = ~df.isin([np.nan, np.inf, -np.inf]).any(1)\n",
    "    return df[indices_to_keep].astype(np.float64)\n",
    "\n",
    "training_data = clean_dataset(training_data)\n",
    "testing_data = clean_dataset(testing_data)\n",
    "#print (testing_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = training_data[['Gender','Race','Ethnic Group','Age_bins', 'Reason1','Reason2','Reason3','Reason4','Reason5','Reason6',\n",
    "        'Reason7','Reason8','Reason9','Reason10','Reason11','Reason12', 'Reason13','Reason14','Reason15','Reason16',\n",
    "         'Reason17','Reason18', 'Reason19','Reason20']]\n",
    "#X_train = training_data[['Gender','Race','Ethnic Group','Age_bins']]\n",
    "y_train = training_data['Class']\n",
    "X_test = testing_data[['Gender','Race','Ethnic Group','Age_bins', 'Reason1','Reason2','Reason3','Reason4','Reason5','Reason6',\n",
    "        'Reason7','Reason8','Reason9','Reason10','Reason11','Reason12', 'Reason13','Reason14','Reason15','Reason16',\n",
    "         'Reason17','Reason18', 'Reason19','Reason20']]\n",
    "#X_test = testing_data[['Gender','Race','Ethnic Group','Age_bins']]\n",
    "y_test = testing_data['Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the K-fold Cross Validator\n",
    "num_folds =10\n",
    "kfold = KFold(n_splits=num_folds, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_133\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_661 (Dense)            (None, 100)               2500      \n",
      "_________________________________________________________________\n",
      "dense_662 (Dense)            (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "dense_663 (Dense)            (None, 25)                1275      \n",
      "_________________________________________________________________\n",
      "dense_664 (Dense)            (None, 10)                260       \n",
      "_________________________________________________________________\n",
      "dense_665 (Dense)            (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 9,096\n",
      "Trainable params: 9,096\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 1 ...\n",
      "Train on 880 samples, validate on 80 samples\n",
      "Epoch 1/50\n",
      "880/880 [==============================] - 3s 4ms/step - loss: 0.6950 - accuracy: 0.5114 - val_loss: 0.7018 - val_accuracy: 0.5000\n",
      "Epoch 2/50\n",
      "880/880 [==============================] - 0s 259us/step - loss: 0.6876 - accuracy: 0.5455 - val_loss: 0.6929 - val_accuracy: 0.5750\n",
      "Epoch 3/50\n",
      "880/880 [==============================] - 0s 261us/step - loss: 0.6862 - accuracy: 0.5557 - val_loss: 0.6964 - val_accuracy: 0.5125\n",
      "Epoch 4/50\n",
      "880/880 [==============================] - 0s 266us/step - loss: 0.6841 - accuracy: 0.5670 - val_loss: 0.6954 - val_accuracy: 0.5625\n",
      "Epoch 5/50\n",
      "880/880 [==============================] - 0s 261us/step - loss: 0.6800 - accuracy: 0.5466 - val_loss: 0.6955 - val_accuracy: 0.5250\n",
      "Epoch 6/50\n",
      "880/880 [==============================] - 0s 260us/step - loss: 0.6771 - accuracy: 0.6011 - val_loss: 0.7075 - val_accuracy: 0.5625\n",
      "Epoch 7/50\n",
      "880/880 [==============================] - 0s 262us/step - loss: 0.6723 - accuracy: 0.5989 - val_loss: 0.7169 - val_accuracy: 0.5625\n",
      "Epoch 8/50\n",
      "880/880 [==============================] - 0s 256us/step - loss: 0.6699 - accuracy: 0.5875 - val_loss: 0.7036 - val_accuracy: 0.5125\n",
      "Epoch 9/50\n",
      "880/880 [==============================] - 0s 251us/step - loss: 0.6656 - accuracy: 0.5943 - val_loss: 0.7077 - val_accuracy: 0.5250\n",
      "Epoch 10/50\n",
      "880/880 [==============================] - 0s 254us/step - loss: 0.6591 - accuracy: 0.5920 - val_loss: 0.7317 - val_accuracy: 0.5125\n",
      "Epoch 11/50\n",
      "880/880 [==============================] - 0s 253us/step - loss: 0.6488 - accuracy: 0.6193 - val_loss: 0.7489 - val_accuracy: 0.5125\n",
      "Epoch 12/50\n",
      "880/880 [==============================] - 0s 251us/step - loss: 0.6423 - accuracy: 0.6284 - val_loss: 0.7661 - val_accuracy: 0.5750\n",
      "Epoch 13/50\n",
      "880/880 [==============================] - 0s 253us/step - loss: 0.6375 - accuracy: 0.6284 - val_loss: 0.7693 - val_accuracy: 0.5250\n",
      "Epoch 14/50\n",
      "880/880 [==============================] - 0s 249us/step - loss: 0.6364 - accuracy: 0.6261 - val_loss: 0.7748 - val_accuracy: 0.4750\n",
      "Epoch 15/50\n",
      "880/880 [==============================] - 0s 251us/step - loss: 0.6310 - accuracy: 0.6375 - val_loss: 0.7765 - val_accuracy: 0.5250\n",
      "Epoch 16/50\n",
      "880/880 [==============================] - 0s 251us/step - loss: 0.6219 - accuracy: 0.6511 - val_loss: 0.8324 - val_accuracy: 0.5125\n",
      "Epoch 17/50\n",
      "880/880 [==============================] - 0s 250us/step - loss: 0.6171 - accuracy: 0.6455 - val_loss: 0.8075 - val_accuracy: 0.4750\n",
      "Epoch 18/50\n",
      "880/880 [==============================] - 0s 252us/step - loss: 0.6110 - accuracy: 0.6409 - val_loss: 0.8399 - val_accuracy: 0.5250\n",
      "Epoch 19/50\n",
      "880/880 [==============================] - 0s 252us/step - loss: 0.6093 - accuracy: 0.6591 - val_loss: 0.8619 - val_accuracy: 0.5125\n",
      "Epoch 20/50\n",
      "880/880 [==============================] - 0s 249us/step - loss: 0.5975 - accuracy: 0.6659 - val_loss: 0.8324 - val_accuracy: 0.5625\n",
      "Epoch 21/50\n",
      "880/880 [==============================] - 0s 250us/step - loss: 0.5933 - accuracy: 0.6602 - val_loss: 0.9035 - val_accuracy: 0.4750\n",
      "Epoch 22/50\n",
      "880/880 [==============================] - 0s 251us/step - loss: 0.5854 - accuracy: 0.6807 - val_loss: 0.9043 - val_accuracy: 0.5000\n",
      "Epoch 23/50\n",
      "880/880 [==============================] - 0s 254us/step - loss: 0.5746 - accuracy: 0.6750 - val_loss: 0.9149 - val_accuracy: 0.4750\n",
      "Epoch 24/50\n",
      "880/880 [==============================] - 0s 257us/step - loss: 0.5797 - accuracy: 0.6852 - val_loss: 0.9616 - val_accuracy: 0.4750\n",
      "Epoch 25/50\n",
      "880/880 [==============================] - 0s 252us/step - loss: 0.5649 - accuracy: 0.7023 - val_loss: 1.0204 - val_accuracy: 0.4375\n",
      "Epoch 26/50\n",
      "880/880 [==============================] - 0s 255us/step - loss: 0.5527 - accuracy: 0.6955 - val_loss: 0.9921 - val_accuracy: 0.5250\n",
      "Epoch 27/50\n",
      "880/880 [==============================] - 0s 252us/step - loss: 0.5537 - accuracy: 0.6943 - val_loss: 0.9497 - val_accuracy: 0.4875\n",
      "Epoch 28/50\n",
      "880/880 [==============================] - 0s 259us/step - loss: 0.5408 - accuracy: 0.7125 - val_loss: 1.0829 - val_accuracy: 0.4625\n",
      "Epoch 29/50\n",
      "880/880 [==============================] - 0s 269us/step - loss: 0.5350 - accuracy: 0.7125 - val_loss: 1.0915 - val_accuracy: 0.5000\n",
      "Epoch 30/50\n",
      "880/880 [==============================] - 0s 263us/step - loss: 0.5388 - accuracy: 0.6864 - val_loss: 1.0794 - val_accuracy: 0.5500\n",
      "Epoch 31/50\n",
      "880/880 [==============================] - 0s 253us/step - loss: 0.5328 - accuracy: 0.6898 - val_loss: 1.1151 - val_accuracy: 0.4875\n",
      "Epoch 32/50\n",
      "880/880 [==============================] - 0s 254us/step - loss: 0.5378 - accuracy: 0.6955 - val_loss: 1.1326 - val_accuracy: 0.5250\n",
      "Epoch 33/50\n",
      "880/880 [==============================] - 0s 253us/step - loss: 0.5306 - accuracy: 0.6932 - val_loss: 1.1428 - val_accuracy: 0.5125\n",
      "Epoch 34/50\n",
      "880/880 [==============================] - 0s 257us/step - loss: 0.5170 - accuracy: 0.6977 - val_loss: 1.1699 - val_accuracy: 0.5125\n",
      "Epoch 35/50\n",
      "880/880 [==============================] - 0s 255us/step - loss: 0.5285 - accuracy: 0.7045 - val_loss: 1.1140 - val_accuracy: 0.4250\n",
      "Epoch 36/50\n",
      "880/880 [==============================] - 0s 255us/step - loss: 0.5097 - accuracy: 0.7193 - val_loss: 1.1737 - val_accuracy: 0.5000\n",
      "Epoch 37/50\n",
      "880/880 [==============================] - 0s 255us/step - loss: 0.5080 - accuracy: 0.7170 - val_loss: 1.1553 - val_accuracy: 0.4875\n",
      "Epoch 38/50\n",
      "880/880 [==============================] - 0s 250us/step - loss: 0.5128 - accuracy: 0.7045 - val_loss: 1.2381 - val_accuracy: 0.5125\n",
      "Epoch 39/50\n",
      "880/880 [==============================] - 0s 256us/step - loss: 0.4963 - accuracy: 0.7239 - val_loss: 1.1930 - val_accuracy: 0.4750\n",
      "Epoch 40/50\n",
      "880/880 [==============================] - 0s 257us/step - loss: 0.5016 - accuracy: 0.6989 - val_loss: 1.2310 - val_accuracy: 0.5125\n",
      "Epoch 41/50\n",
      "880/880 [==============================] - 0s 254us/step - loss: 0.5003 - accuracy: 0.7193 - val_loss: 1.3736 - val_accuracy: 0.5125\n",
      "Epoch 42/50\n",
      "880/880 [==============================] - 0s 258us/step - loss: 0.5093 - accuracy: 0.7193 - val_loss: 1.2886 - val_accuracy: 0.4750\n",
      "Epoch 43/50\n",
      "880/880 [==============================] - 0s 256us/step - loss: 0.4922 - accuracy: 0.7170 - val_loss: 1.3196 - val_accuracy: 0.5375\n",
      "Epoch 44/50\n",
      "880/880 [==============================] - 0s 255us/step - loss: 0.4955 - accuracy: 0.7102 - val_loss: 1.3630 - val_accuracy: 0.4625\n",
      "Epoch 45/50\n",
      "880/880 [==============================] - 0s 254us/step - loss: 0.4895 - accuracy: 0.7295 - val_loss: 1.3039 - val_accuracy: 0.4625\n",
      "Epoch 46/50\n",
      "880/880 [==============================] - 0s 256us/step - loss: 0.4968 - accuracy: 0.7091 - val_loss: 1.3126 - val_accuracy: 0.4500\n",
      "Epoch 47/50\n",
      "880/880 [==============================] - 0s 254us/step - loss: 0.5037 - accuracy: 0.7148 - val_loss: 1.3905 - val_accuracy: 0.5250\n",
      "Epoch 48/50\n",
      "880/880 [==============================] - 0s 257us/step - loss: 0.5067 - accuracy: 0.7227 - val_loss: 1.2780 - val_accuracy: 0.4750\n",
      "Epoch 49/50\n",
      "880/880 [==============================] - 0s 255us/step - loss: 0.4858 - accuracy: 0.7273 - val_loss: 1.3778 - val_accuracy: 0.5000\n",
      "Epoch 50/50\n",
      "880/880 [==============================] - 0s 259us/step - loss: 0.4858 - accuracy: 0.7318 - val_loss: 1.2841 - val_accuracy: 0.5000\n",
      "Score for fold 1: loss of 1.2841314792633056; accuracy of 50.0%\n",
      "Model: \"sequential_134\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_666 (Dense)            (None, 100)               2500      \n",
      "_________________________________________________________________\n",
      "dense_667 (Dense)            (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "dense_668 (Dense)            (None, 25)                1275      \n",
      "_________________________________________________________________\n",
      "dense_669 (Dense)            (None, 10)                260       \n",
      "_________________________________________________________________\n",
      "dense_670 (Dense)            (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 9,096\n",
      "Trainable params: 9,096\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 2 ...\n",
      "Train on 880 samples, validate on 80 samples\n",
      "Epoch 1/50\n",
      "880/880 [==============================] - 3s 4ms/step - loss: 0.6909 - accuracy: 0.5330 - val_loss: 0.6918 - val_accuracy: 0.5125\n",
      "Epoch 2/50\n",
      "880/880 [==============================] - 0s 279us/step - loss: 0.6899 - accuracy: 0.5250 - val_loss: 0.6902 - val_accuracy: 0.5500\n",
      "Epoch 3/50\n",
      "880/880 [==============================] - 0s 281us/step - loss: 0.6869 - accuracy: 0.5409 - val_loss: 0.6881 - val_accuracy: 0.5500\n",
      "Epoch 4/50\n",
      "880/880 [==============================] - 0s 285us/step - loss: 0.6826 - accuracy: 0.5455 - val_loss: 0.6882 - val_accuracy: 0.5375\n",
      "Epoch 5/50\n",
      "880/880 [==============================] - 0s 255us/step - loss: 0.6796 - accuracy: 0.5852 - val_loss: 0.6855 - val_accuracy: 0.6125\n",
      "Epoch 6/50\n",
      "880/880 [==============================] - 0s 253us/step - loss: 0.6766 - accuracy: 0.5761 - val_loss: 0.6897 - val_accuracy: 0.5625\n",
      "Epoch 7/50\n",
      "880/880 [==============================] - 0s 256us/step - loss: 0.6728 - accuracy: 0.5943 - val_loss: 0.6962 - val_accuracy: 0.5625\n",
      "Epoch 8/50\n",
      "880/880 [==============================] - 0s 252us/step - loss: 0.6745 - accuracy: 0.5818 - val_loss: 0.6903 - val_accuracy: 0.5625\n",
      "Epoch 9/50\n",
      "880/880 [==============================] - 0s 251us/step - loss: 0.6647 - accuracy: 0.6125 - val_loss: 0.6965 - val_accuracy: 0.5125\n",
      "Epoch 10/50\n",
      "880/880 [==============================] - 0s 250us/step - loss: 0.6587 - accuracy: 0.6148 - val_loss: 0.7008 - val_accuracy: 0.5375\n",
      "Epoch 11/50\n",
      "880/880 [==============================] - 0s 253us/step - loss: 0.6542 - accuracy: 0.6148 - val_loss: 0.7142 - val_accuracy: 0.5250\n",
      "Epoch 12/50\n",
      "880/880 [==============================] - 0s 250us/step - loss: 0.6492 - accuracy: 0.6193 - val_loss: 0.7235 - val_accuracy: 0.4875\n",
      "Epoch 13/50\n",
      "880/880 [==============================] - 0s 255us/step - loss: 0.6477 - accuracy: 0.6068 - val_loss: 0.7202 - val_accuracy: 0.5250\n",
      "Epoch 14/50\n",
      "880/880 [==============================] - 0s 249us/step - loss: 0.6348 - accuracy: 0.6420 - val_loss: 0.7472 - val_accuracy: 0.5125\n",
      "Epoch 15/50\n",
      "880/880 [==============================] - 0s 255us/step - loss: 0.6269 - accuracy: 0.6455 - val_loss: 0.7568 - val_accuracy: 0.5250\n",
      "Epoch 16/50\n",
      "880/880 [==============================] - 0s 252us/step - loss: 0.6201 - accuracy: 0.6545 - val_loss: 0.7646 - val_accuracy: 0.4875\n",
      "Epoch 17/50\n",
      "880/880 [==============================] - 0s 258us/step - loss: 0.6144 - accuracy: 0.6648 - val_loss: 0.7916 - val_accuracy: 0.5125\n",
      "Epoch 18/50\n",
      "880/880 [==============================] - 0s 253us/step - loss: 0.6088 - accuracy: 0.6659 - val_loss: 0.8271 - val_accuracy: 0.5125\n",
      "Epoch 19/50\n",
      "880/880 [==============================] - 0s 252us/step - loss: 0.6105 - accuracy: 0.6432 - val_loss: 0.8142 - val_accuracy: 0.4750\n",
      "Epoch 20/50\n",
      "880/880 [==============================] - 0s 251us/step - loss: 0.5923 - accuracy: 0.6830 - val_loss: 0.8596 - val_accuracy: 0.5000\n",
      "Epoch 21/50\n",
      "880/880 [==============================] - 0s 252us/step - loss: 0.5979 - accuracy: 0.6727 - val_loss: 0.8400 - val_accuracy: 0.5125\n",
      "Epoch 22/50\n",
      "880/880 [==============================] - 0s 252us/step - loss: 0.5806 - accuracy: 0.6727 - val_loss: 0.8890 - val_accuracy: 0.4625\n",
      "Epoch 23/50\n",
      "880/880 [==============================] - 0s 255us/step - loss: 0.5706 - accuracy: 0.6932 - val_loss: 0.9191 - val_accuracy: 0.4875\n",
      "Epoch 24/50\n",
      "880/880 [==============================] - 0s 253us/step - loss: 0.5845 - accuracy: 0.6795 - val_loss: 0.8902 - val_accuracy: 0.5000\n",
      "Epoch 25/50\n",
      "880/880 [==============================] - 0s 251us/step - loss: 0.5699 - accuracy: 0.6795 - val_loss: 0.9325 - val_accuracy: 0.4375\n",
      "Epoch 26/50\n",
      "880/880 [==============================] - 0s 254us/step - loss: 0.5664 - accuracy: 0.7011 - val_loss: 0.9363 - val_accuracy: 0.4750\n",
      "Epoch 27/50\n",
      "880/880 [==============================] - 0s 253us/step - loss: 0.5466 - accuracy: 0.7114 - val_loss: 0.9446 - val_accuracy: 0.4750\n",
      "Epoch 28/50\n",
      "880/880 [==============================] - 0s 256us/step - loss: 0.5402 - accuracy: 0.7091 - val_loss: 0.9526 - val_accuracy: 0.4625\n",
      "Epoch 29/50\n",
      "880/880 [==============================] - 0s 263us/step - loss: 0.5369 - accuracy: 0.7080 - val_loss: 0.9698 - val_accuracy: 0.5000\n",
      "Epoch 30/50\n",
      "880/880 [==============================] - 0s 257us/step - loss: 0.5214 - accuracy: 0.7170 - val_loss: 1.0207 - val_accuracy: 0.4750\n",
      "Epoch 31/50\n",
      "880/880 [==============================] - 0s 254us/step - loss: 0.5228 - accuracy: 0.7205 - val_loss: 1.0324 - val_accuracy: 0.4625\n",
      "Epoch 32/50\n",
      "880/880 [==============================] - 0s 256us/step - loss: 0.5243 - accuracy: 0.7227 - val_loss: 1.0069 - val_accuracy: 0.4750\n",
      "Epoch 33/50\n",
      "880/880 [==============================] - 0s 255us/step - loss: 0.5164 - accuracy: 0.7136 - val_loss: 1.0685 - val_accuracy: 0.5000\n",
      "Epoch 34/50\n",
      "880/880 [==============================] - 0s 251us/step - loss: 0.5068 - accuracy: 0.7125 - val_loss: 1.0320 - val_accuracy: 0.5375\n",
      "Epoch 35/50\n",
      "880/880 [==============================] - 0s 252us/step - loss: 0.5092 - accuracy: 0.7250 - val_loss: 1.1151 - val_accuracy: 0.5500\n",
      "Epoch 36/50\n",
      "880/880 [==============================] - 0s 258us/step - loss: 0.5223 - accuracy: 0.7057 - val_loss: 1.0023 - val_accuracy: 0.4750\n",
      "Epoch 37/50\n",
      "880/880 [==============================] - 0s 254us/step - loss: 0.5095 - accuracy: 0.7216 - val_loss: 1.0572 - val_accuracy: 0.4750\n",
      "Epoch 38/50\n",
      "880/880 [==============================] - 0s 254us/step - loss: 0.4951 - accuracy: 0.7352 - val_loss: 1.0956 - val_accuracy: 0.4625\n",
      "Epoch 39/50\n",
      "880/880 [==============================] - 0s 254us/step - loss: 0.4937 - accuracy: 0.7352 - val_loss: 1.0678 - val_accuracy: 0.5500\n",
      "Epoch 40/50\n",
      "880/880 [==============================] - 0s 250us/step - loss: 0.4909 - accuracy: 0.7318 - val_loss: 1.1114 - val_accuracy: 0.5625\n",
      "Epoch 41/50\n",
      "880/880 [==============================] - 0s 253us/step - loss: 0.4904 - accuracy: 0.7261 - val_loss: 1.1441 - val_accuracy: 0.5250\n",
      "Epoch 42/50\n",
      "880/880 [==============================] - 0s 266us/step - loss: 0.4887 - accuracy: 0.7227 - val_loss: 1.1606 - val_accuracy: 0.5375\n",
      "Epoch 43/50\n",
      "880/880 [==============================] - 0s 258us/step - loss: 0.4823 - accuracy: 0.7364 - val_loss: 1.1540 - val_accuracy: 0.5375\n",
      "Epoch 44/50\n",
      "880/880 [==============================] - 0s 251us/step - loss: 0.4821 - accuracy: 0.7398 - val_loss: 1.2188 - val_accuracy: 0.5375\n",
      "Epoch 45/50\n",
      "880/880 [==============================] - 0s 261us/step - loss: 0.4830 - accuracy: 0.7250 - val_loss: 1.1711 - val_accuracy: 0.5250\n",
      "Epoch 46/50\n",
      "880/880 [==============================] - 0s 262us/step - loss: 0.4846 - accuracy: 0.7364 - val_loss: 1.2132 - val_accuracy: 0.5375\n",
      "Epoch 47/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "880/880 [==============================] - 0s 257us/step - loss: 0.4847 - accuracy: 0.7250 - val_loss: 1.1804 - val_accuracy: 0.5375\n",
      "Epoch 48/50\n",
      "880/880 [==============================] - 0s 250us/step - loss: 0.4800 - accuracy: 0.7398 - val_loss: 1.2202 - val_accuracy: 0.5375\n",
      "Epoch 49/50\n",
      "880/880 [==============================] - 0s 257us/step - loss: 0.4772 - accuracy: 0.7352 - val_loss: 1.2686 - val_accuracy: 0.5125\n",
      "Epoch 50/50\n",
      "880/880 [==============================] - 0s 261us/step - loss: 0.4878 - accuracy: 0.7250 - val_loss: 1.2253 - val_accuracy: 0.5250\n",
      "Score for fold 2: loss of 1.2253441095352173; accuracy of 52.49999761581421%\n",
      "Model: \"sequential_135\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_671 (Dense)            (None, 100)               2500      \n",
      "_________________________________________________________________\n",
      "dense_672 (Dense)            (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "dense_673 (Dense)            (None, 25)                1275      \n",
      "_________________________________________________________________\n",
      "dense_674 (Dense)            (None, 10)                260       \n",
      "_________________________________________________________________\n",
      "dense_675 (Dense)            (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 9,096\n",
      "Trainable params: 9,096\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 3 ...\n",
      "Train on 880 samples, validate on 80 samples\n",
      "Epoch 1/50\n",
      "880/880 [==============================] - 3s 4ms/step - loss: 0.6952 - accuracy: 0.5159 - val_loss: 0.6935 - val_accuracy: 0.4750\n",
      "Epoch 2/50\n",
      "880/880 [==============================] - 0s 283us/step - loss: 0.6895 - accuracy: 0.5398 - val_loss: 0.6896 - val_accuracy: 0.6125\n",
      "Epoch 3/50\n",
      "880/880 [==============================] - 0s 287us/step - loss: 0.6872 - accuracy: 0.5568 - val_loss: 0.6898 - val_accuracy: 0.5500\n",
      "Epoch 4/50\n",
      "880/880 [==============================] - 0s 284us/step - loss: 0.6846 - accuracy: 0.5716 - val_loss: 0.6880 - val_accuracy: 0.5875\n",
      "Epoch 5/50\n",
      "880/880 [==============================] - 0s 284us/step - loss: 0.6833 - accuracy: 0.5818 - val_loss: 0.6883 - val_accuracy: 0.5875\n",
      "Epoch 6/50\n",
      "880/880 [==============================] - 0s 281us/step - loss: 0.6799 - accuracy: 0.5841 - val_loss: 0.6897 - val_accuracy: 0.5250\n",
      "Epoch 7/50\n",
      "880/880 [==============================] - 0s 281us/step - loss: 0.6796 - accuracy: 0.5807 - val_loss: 0.6923 - val_accuracy: 0.5500\n",
      "Epoch 8/50\n",
      "880/880 [==============================] - 0s 279us/step - loss: 0.6720 - accuracy: 0.5943 - val_loss: 0.7018 - val_accuracy: 0.5250\n",
      "Epoch 9/50\n",
      "880/880 [==============================] - 0s 279us/step - loss: 0.6723 - accuracy: 0.5977 - val_loss: 0.6976 - val_accuracy: 0.5125\n",
      "Epoch 10/50\n",
      "880/880 [==============================] - 0s 283us/step - loss: 0.6711 - accuracy: 0.5818 - val_loss: 0.7043 - val_accuracy: 0.4625\n",
      "Epoch 11/50\n",
      "880/880 [==============================] - 0s 280us/step - loss: 0.6645 - accuracy: 0.6136 - val_loss: 0.7011 - val_accuracy: 0.5125\n",
      "Epoch 12/50\n",
      "880/880 [==============================] - 0s 279us/step - loss: 0.6581 - accuracy: 0.6284 - val_loss: 0.7042 - val_accuracy: 0.5375\n",
      "Epoch 13/50\n",
      "880/880 [==============================] - 0s 280us/step - loss: 0.6544 - accuracy: 0.6261 - val_loss: 0.7036 - val_accuracy: 0.5375\n",
      "Epoch 14/50\n",
      "880/880 [==============================] - 0s 281us/step - loss: 0.6455 - accuracy: 0.6386 - val_loss: 0.7208 - val_accuracy: 0.4875\n",
      "Epoch 15/50\n",
      "880/880 [==============================] - 0s 284us/step - loss: 0.6369 - accuracy: 0.6534 - val_loss: 0.7048 - val_accuracy: 0.5375\n",
      "Epoch 16/50\n",
      "880/880 [==============================] - 0s 276us/step - loss: 0.6404 - accuracy: 0.6477 - val_loss: 0.7180 - val_accuracy: 0.5375\n",
      "Epoch 17/50\n",
      "880/880 [==============================] - 0s 283us/step - loss: 0.6253 - accuracy: 0.6614 - val_loss: 0.7121 - val_accuracy: 0.5375\n",
      "Epoch 18/50\n",
      "880/880 [==============================] - 0s 280us/step - loss: 0.6160 - accuracy: 0.6750 - val_loss: 0.7305 - val_accuracy: 0.5250\n",
      "Epoch 19/50\n",
      "880/880 [==============================] - 0s 282us/step - loss: 0.6158 - accuracy: 0.6716 - val_loss: 0.7473 - val_accuracy: 0.5500\n",
      "Epoch 20/50\n",
      "880/880 [==============================] - 0s 277us/step - loss: 0.6170 - accuracy: 0.6591 - val_loss: 0.7332 - val_accuracy: 0.5250\n",
      "Epoch 21/50\n",
      "880/880 [==============================] - 0s 280us/step - loss: 0.6028 - accuracy: 0.6648 - val_loss: 0.7363 - val_accuracy: 0.5500\n",
      "Epoch 22/50\n",
      "880/880 [==============================] - 0s 277us/step - loss: 0.5846 - accuracy: 0.6909 - val_loss: 0.7602 - val_accuracy: 0.5125\n",
      "Epoch 23/50\n",
      "880/880 [==============================] - 0s 285us/step - loss: 0.5776 - accuracy: 0.6727 - val_loss: 0.7683 - val_accuracy: 0.5125\n",
      "Epoch 24/50\n",
      "880/880 [==============================] - 0s 278us/step - loss: 0.5761 - accuracy: 0.6784 - val_loss: 0.7678 - val_accuracy: 0.5250\n",
      "Epoch 25/50\n",
      "880/880 [==============================] - 0s 277us/step - loss: 0.5785 - accuracy: 0.6773 - val_loss: 0.7535 - val_accuracy: 0.4875\n",
      "Epoch 26/50\n",
      "880/880 [==============================] - 0s 280us/step - loss: 0.5720 - accuracy: 0.6795 - val_loss: 0.7917 - val_accuracy: 0.5375\n",
      "Epoch 27/50\n",
      "880/880 [==============================] - 0s 280us/step - loss: 0.5618 - accuracy: 0.7011 - val_loss: 0.7797 - val_accuracy: 0.5500\n",
      "Epoch 28/50\n",
      "880/880 [==============================] - 0s 284us/step - loss: 0.5482 - accuracy: 0.7057 - val_loss: 0.8153 - val_accuracy: 0.5125\n",
      "Epoch 29/50\n",
      "880/880 [==============================] - 0s 280us/step - loss: 0.5645 - accuracy: 0.6932 - val_loss: 0.8271 - val_accuracy: 0.5125\n",
      "Epoch 30/50\n",
      "880/880 [==============================] - 0s 282us/step - loss: 0.5623 - accuracy: 0.6852 - val_loss: 0.8022 - val_accuracy: 0.5625\n",
      "Epoch 31/50\n",
      "880/880 [==============================] - 0s 290us/step - loss: 0.5426 - accuracy: 0.6989 - val_loss: 0.8213 - val_accuracy: 0.5000\n",
      "Epoch 32/50\n",
      "880/880 [==============================] - 0s 280us/step - loss: 0.5332 - accuracy: 0.7011 - val_loss: 0.8146 - val_accuracy: 0.5375\n",
      "Epoch 33/50\n",
      "880/880 [==============================] - 0s 283us/step - loss: 0.5334 - accuracy: 0.7091 - val_loss: 0.8513 - val_accuracy: 0.5250\n",
      "Epoch 34/50\n",
      "880/880 [==============================] - 0s 285us/step - loss: 0.5333 - accuracy: 0.7034 - val_loss: 0.8517 - val_accuracy: 0.5250\n",
      "Epoch 35/50\n",
      "880/880 [==============================] - 0s 286us/step - loss: 0.5302 - accuracy: 0.7102 - val_loss: 0.8829 - val_accuracy: 0.5125\n",
      "Epoch 36/50\n",
      "880/880 [==============================] - 0s 281us/step - loss: 0.5414 - accuracy: 0.7125 - val_loss: 0.8666 - val_accuracy: 0.4750\n",
      "Epoch 37/50\n",
      "880/880 [==============================] - 0s 281us/step - loss: 0.5328 - accuracy: 0.7216 - val_loss: 0.8477 - val_accuracy: 0.5625\n",
      "Epoch 38/50\n",
      "880/880 [==============================] - 0s 286us/step - loss: 0.5144 - accuracy: 0.7193 - val_loss: 0.8908 - val_accuracy: 0.4750\n",
      "Epoch 39/50\n",
      "880/880 [==============================] - 0s 284us/step - loss: 0.5084 - accuracy: 0.7295 - val_loss: 0.9165 - val_accuracy: 0.5125\n",
      "Epoch 40/50\n",
      "880/880 [==============================] - 0s 282us/step - loss: 0.5054 - accuracy: 0.7341 - val_loss: 0.9552 - val_accuracy: 0.5000\n",
      "Epoch 41/50\n",
      "880/880 [==============================] - 0s 284us/step - loss: 0.5155 - accuracy: 0.7068 - val_loss: 0.9375 - val_accuracy: 0.5000\n",
      "Epoch 42/50\n",
      "880/880 [==============================] - 0s 281us/step - loss: 0.5178 - accuracy: 0.7250 - val_loss: 0.9006 - val_accuracy: 0.5000\n",
      "Epoch 43/50\n",
      "880/880 [==============================] - 0s 282us/step - loss: 0.5023 - accuracy: 0.7341 - val_loss: 0.9188 - val_accuracy: 0.5500\n",
      "Epoch 44/50\n",
      "880/880 [==============================] - 0s 282us/step - loss: 0.5091 - accuracy: 0.7136 - val_loss: 0.9474 - val_accuracy: 0.4875\n",
      "Epoch 45/50\n",
      "880/880 [==============================] - 0s 281us/step - loss: 0.4957 - accuracy: 0.7307 - val_loss: 0.9544 - val_accuracy: 0.5125\n",
      "Epoch 46/50\n",
      "880/880 [==============================] - 0s 281us/step - loss: 0.4912 - accuracy: 0.7341 - val_loss: 0.9870 - val_accuracy: 0.5375\n",
      "Epoch 47/50\n",
      "880/880 [==============================] - 0s 283us/step - loss: 0.5029 - accuracy: 0.7159 - val_loss: 0.9501 - val_accuracy: 0.5250\n",
      "Epoch 48/50\n",
      "880/880 [==============================] - 0s 281us/step - loss: 0.4991 - accuracy: 0.7227 - val_loss: 0.9819 - val_accuracy: 0.5250\n",
      "Epoch 49/50\n",
      "880/880 [==============================] - 0s 281us/step - loss: 0.4992 - accuracy: 0.7250 - val_loss: 0.9684 - val_accuracy: 0.5125\n",
      "Epoch 50/50\n",
      "880/880 [==============================] - 0s 282us/step - loss: 0.4908 - accuracy: 0.7284 - val_loss: 1.0091 - val_accuracy: 0.5500\n",
      "Score for fold 3: loss of 1.0090792417526244; accuracy of 55.000001192092896%\n",
      "Model: \"sequential_136\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_676 (Dense)            (None, 100)               2500      \n",
      "_________________________________________________________________\n",
      "dense_677 (Dense)            (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "dense_678 (Dense)            (None, 25)                1275      \n",
      "_________________________________________________________________\n",
      "dense_679 (Dense)            (None, 10)                260       \n",
      "_________________________________________________________________\n",
      "dense_680 (Dense)            (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 9,096\n",
      "Trainable params: 9,096\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 4 ...\n",
      "Train on 880 samples, validate on 80 samples\n",
      "Epoch 1/50\n",
      "880/880 [==============================] - 3s 4ms/step - loss: 0.6923 - accuracy: 0.5466 - val_loss: 0.6901 - val_accuracy: 0.5250\n",
      "Epoch 2/50\n",
      "880/880 [==============================] - 0s 284us/step - loss: 0.6900 - accuracy: 0.5386 - val_loss: 0.6904 - val_accuracy: 0.5375\n",
      "Epoch 3/50\n",
      "880/880 [==============================] - 0s 288us/step - loss: 0.6860 - accuracy: 0.5398 - val_loss: 0.6972 - val_accuracy: 0.5250\n",
      "Epoch 4/50\n",
      "880/880 [==============================] - 0s 286us/step - loss: 0.6854 - accuracy: 0.5534 - val_loss: 0.6984 - val_accuracy: 0.5375\n",
      "Epoch 5/50\n",
      "880/880 [==============================] - 0s 288us/step - loss: 0.6787 - accuracy: 0.5727 - val_loss: 0.6940 - val_accuracy: 0.4750\n",
      "Epoch 6/50\n",
      "880/880 [==============================] - 0s 283us/step - loss: 0.6763 - accuracy: 0.5761 - val_loss: 0.6935 - val_accuracy: 0.5125\n",
      "Epoch 7/50\n",
      "880/880 [==============================] - 0s 285us/step - loss: 0.6682 - accuracy: 0.5989 - val_loss: 0.7071 - val_accuracy: 0.5875\n",
      "Epoch 8/50\n",
      "880/880 [==============================] - 0s 285us/step - loss: 0.6656 - accuracy: 0.5966 - val_loss: 0.7157 - val_accuracy: 0.5750\n",
      "Epoch 9/50\n",
      "880/880 [==============================] - 0s 287us/step - loss: 0.6610 - accuracy: 0.6125 - val_loss: 0.7117 - val_accuracy: 0.5000\n",
      "Epoch 10/50\n",
      "880/880 [==============================] - 0s 286us/step - loss: 0.6608 - accuracy: 0.6045 - val_loss: 0.7154 - val_accuracy: 0.5375\n",
      "Epoch 11/50\n",
      "880/880 [==============================] - 0s 284us/step - loss: 0.6553 - accuracy: 0.6057 - val_loss: 0.7257 - val_accuracy: 0.5625\n",
      "Epoch 12/50\n",
      "880/880 [==============================] - 0s 286us/step - loss: 0.6486 - accuracy: 0.6250 - val_loss: 0.7326 - val_accuracy: 0.4750\n",
      "Epoch 13/50\n",
      "880/880 [==============================] - 0s 284us/step - loss: 0.6443 - accuracy: 0.6216 - val_loss: 0.7460 - val_accuracy: 0.5125\n",
      "Epoch 14/50\n",
      "880/880 [==============================] - 0s 284us/step - loss: 0.6394 - accuracy: 0.6386 - val_loss: 0.7677 - val_accuracy: 0.4625\n",
      "Epoch 15/50\n",
      "880/880 [==============================] - 0s 284us/step - loss: 0.6323 - accuracy: 0.6386 - val_loss: 0.7623 - val_accuracy: 0.4875\n",
      "Epoch 16/50\n",
      "880/880 [==============================] - 0s 284us/step - loss: 0.6244 - accuracy: 0.6545 - val_loss: 0.7817 - val_accuracy: 0.4500\n",
      "Epoch 17/50\n",
      "880/880 [==============================] - 0s 282us/step - loss: 0.6238 - accuracy: 0.6545 - val_loss: 0.8025 - val_accuracy: 0.4875\n",
      "Epoch 18/50\n",
      "880/880 [==============================] - 0s 283us/step - loss: 0.6185 - accuracy: 0.6352 - val_loss: 0.8199 - val_accuracy: 0.4625\n",
      "Epoch 19/50\n",
      "880/880 [==============================] - 0s 281us/step - loss: 0.6156 - accuracy: 0.6477 - val_loss: 0.8016 - val_accuracy: 0.4750\n",
      "Epoch 20/50\n",
      "880/880 [==============================] - 0s 282us/step - loss: 0.6109 - accuracy: 0.6420 - val_loss: 0.8790 - val_accuracy: 0.5500\n",
      "Epoch 21/50\n",
      "880/880 [==============================] - 0s 284us/step - loss: 0.6071 - accuracy: 0.6648 - val_loss: 0.8672 - val_accuracy: 0.4625\n",
      "Epoch 22/50\n",
      "880/880 [==============================] - 0s 284us/step - loss: 0.5924 - accuracy: 0.6818 - val_loss: 0.8485 - val_accuracy: 0.4875\n",
      "Epoch 23/50\n",
      "880/880 [==============================] - 0s 282us/step - loss: 0.5762 - accuracy: 0.7011 - val_loss: 0.9393 - val_accuracy: 0.5375\n",
      "Epoch 24/50\n",
      "880/880 [==============================] - 0s 283us/step - loss: 0.5859 - accuracy: 0.6784 - val_loss: 0.8677 - val_accuracy: 0.4750\n",
      "Epoch 25/50\n",
      "880/880 [==============================] - 0s 283us/step - loss: 0.5730 - accuracy: 0.6920 - val_loss: 0.9026 - val_accuracy: 0.4750\n",
      "Epoch 26/50\n",
      "880/880 [==============================] - 0s 283us/step - loss: 0.5795 - accuracy: 0.6818 - val_loss: 0.9483 - val_accuracy: 0.4750\n",
      "Epoch 27/50\n",
      "880/880 [==============================] - 0s 283us/step - loss: 0.5706 - accuracy: 0.6852 - val_loss: 0.9188 - val_accuracy: 0.4625\n",
      "Epoch 28/50\n",
      "880/880 [==============================] - 0s 285us/step - loss: 0.5624 - accuracy: 0.7034 - val_loss: 0.8975 - val_accuracy: 0.4750\n",
      "Epoch 29/50\n",
      "880/880 [==============================] - 0s 283us/step - loss: 0.5481 - accuracy: 0.7136 - val_loss: 0.9033 - val_accuracy: 0.4750\n",
      "Epoch 30/50\n",
      "880/880 [==============================] - 0s 282us/step - loss: 0.5446 - accuracy: 0.7068 - val_loss: 0.9401 - val_accuracy: 0.5125\n",
      "Epoch 31/50\n",
      "880/880 [==============================] - 0s 284us/step - loss: 0.5324 - accuracy: 0.7159 - val_loss: 0.9454 - val_accuracy: 0.4750\n",
      "Epoch 32/50\n",
      "880/880 [==============================] - 0s 284us/step - loss: 0.5418 - accuracy: 0.6875 - val_loss: 0.9375 - val_accuracy: 0.4750\n",
      "Epoch 33/50\n",
      "880/880 [==============================] - 0s 286us/step - loss: 0.5265 - accuracy: 0.7080 - val_loss: 0.9630 - val_accuracy: 0.4750\n",
      "Epoch 34/50\n",
      "880/880 [==============================] - 0s 288us/step - loss: 0.5183 - accuracy: 0.7239 - val_loss: 0.9766 - val_accuracy: 0.5000\n",
      "Epoch 35/50\n",
      "880/880 [==============================] - 0s 283us/step - loss: 0.5237 - accuracy: 0.7034 - val_loss: 1.0201 - val_accuracy: 0.5000\n",
      "Epoch 36/50\n",
      "880/880 [==============================] - 0s 281us/step - loss: 0.5224 - accuracy: 0.6932 - val_loss: 0.9825 - val_accuracy: 0.5000\n",
      "Epoch 37/50\n",
      "880/880 [==============================] - 0s 284us/step - loss: 0.5054 - accuracy: 0.7193 - val_loss: 1.0814 - val_accuracy: 0.4750\n",
      "Epoch 38/50\n",
      "880/880 [==============================] - 0s 282us/step - loss: 0.5065 - accuracy: 0.7284 - val_loss: 1.0308 - val_accuracy: 0.4875\n",
      "Epoch 39/50\n",
      "880/880 [==============================] - 0s 280us/step - loss: 0.5112 - accuracy: 0.7205 - val_loss: 1.0888 - val_accuracy: 0.5125\n",
      "Epoch 40/50\n",
      "880/880 [==============================] - 0s 282us/step - loss: 0.5153 - accuracy: 0.7170 - val_loss: 1.0621 - val_accuracy: 0.5375\n",
      "Epoch 41/50\n",
      "880/880 [==============================] - 0s 283us/step - loss: 0.5033 - accuracy: 0.7182 - val_loss: 1.0787 - val_accuracy: 0.4750\n",
      "Epoch 42/50\n",
      "880/880 [==============================] - 0s 287us/step - loss: 0.4978 - accuracy: 0.7239 - val_loss: 1.1096 - val_accuracy: 0.5250\n",
      "Epoch 43/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "880/880 [==============================] - 0s 284us/step - loss: 0.5072 - accuracy: 0.7239 - val_loss: 1.0441 - val_accuracy: 0.4625\n",
      "Epoch 44/50\n",
      "880/880 [==============================] - 0s 288us/step - loss: 0.5052 - accuracy: 0.7250 - val_loss: 1.1084 - val_accuracy: 0.5000\n",
      "Epoch 45/50\n",
      "880/880 [==============================] - 0s 280us/step - loss: 0.4951 - accuracy: 0.7352 - val_loss: 1.1038 - val_accuracy: 0.5250\n",
      "Epoch 46/50\n",
      "880/880 [==============================] - 0s 283us/step - loss: 0.4890 - accuracy: 0.7330 - val_loss: 1.1241 - val_accuracy: 0.4625\n",
      "Epoch 47/50\n",
      "880/880 [==============================] - 0s 283us/step - loss: 0.4808 - accuracy: 0.7375 - val_loss: 1.0826 - val_accuracy: 0.5250\n",
      "Epoch 48/50\n",
      "880/880 [==============================] - 0s 281us/step - loss: 0.4913 - accuracy: 0.7193 - val_loss: 1.1106 - val_accuracy: 0.4875\n",
      "Epoch 49/50\n",
      "880/880 [==============================] - 0s 282us/step - loss: 0.4833 - accuracy: 0.7352 - val_loss: 1.1590 - val_accuracy: 0.5250\n",
      "Epoch 50/50\n",
      "880/880 [==============================] - 0s 280us/step - loss: 0.4936 - accuracy: 0.7159 - val_loss: 1.1714 - val_accuracy: 0.5000\n",
      "Score for fold 4: loss of 1.1713934898376466; accuracy of 50.0%\n",
      "Model: \"sequential_137\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_681 (Dense)            (None, 100)               2500      \n",
      "_________________________________________________________________\n",
      "dense_682 (Dense)            (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "dense_683 (Dense)            (None, 25)                1275      \n",
      "_________________________________________________________________\n",
      "dense_684 (Dense)            (None, 10)                260       \n",
      "_________________________________________________________________\n",
      "dense_685 (Dense)            (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 9,096\n",
      "Trainable params: 9,096\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 5 ...\n",
      "Train on 880 samples, validate on 80 samples\n",
      "Epoch 1/50\n",
      "880/880 [==============================] - 3s 4ms/step - loss: 0.6946 - accuracy: 0.5148 - val_loss: 0.6947 - val_accuracy: 0.5125\n",
      "Epoch 2/50\n",
      "880/880 [==============================] - 0s 286us/step - loss: 0.6887 - accuracy: 0.5307 - val_loss: 0.6943 - val_accuracy: 0.5125\n",
      "Epoch 3/50\n",
      "880/880 [==============================] - 0s 288us/step - loss: 0.6853 - accuracy: 0.5477 - val_loss: 0.6908 - val_accuracy: 0.5625\n",
      "Epoch 4/50\n",
      "880/880 [==============================] - 0s 288us/step - loss: 0.6804 - accuracy: 0.5739 - val_loss: 0.6955 - val_accuracy: 0.5125\n",
      "Epoch 5/50\n",
      "880/880 [==============================] - 0s 287us/step - loss: 0.6759 - accuracy: 0.5636 - val_loss: 0.6918 - val_accuracy: 0.5875\n",
      "Epoch 6/50\n",
      "880/880 [==============================] - 0s 284us/step - loss: 0.6693 - accuracy: 0.6000 - val_loss: 0.7132 - val_accuracy: 0.5500\n",
      "Epoch 7/50\n",
      "880/880 [==============================] - 0s 287us/step - loss: 0.6700 - accuracy: 0.5852 - val_loss: 0.7083 - val_accuracy: 0.5625\n",
      "Epoch 8/50\n",
      "880/880 [==============================] - 0s 286us/step - loss: 0.6656 - accuracy: 0.5989 - val_loss: 0.7053 - val_accuracy: 0.5000\n",
      "Epoch 9/50\n",
      "880/880 [==============================] - 0s 287us/step - loss: 0.6576 - accuracy: 0.6125 - val_loss: 0.7250 - val_accuracy: 0.5375\n",
      "Epoch 10/50\n",
      "880/880 [==============================] - 0s 284us/step - loss: 0.6598 - accuracy: 0.6159 - val_loss: 0.7132 - val_accuracy: 0.5500\n",
      "Epoch 11/50\n",
      "880/880 [==============================] - 0s 287us/step - loss: 0.6493 - accuracy: 0.6205 - val_loss: 0.7373 - val_accuracy: 0.5125\n",
      "Epoch 12/50\n",
      "880/880 [==============================] - 0s 287us/step - loss: 0.6451 - accuracy: 0.6341 - val_loss: 0.7277 - val_accuracy: 0.5125\n",
      "Epoch 13/50\n",
      "880/880 [==============================] - 0s 287us/step - loss: 0.6393 - accuracy: 0.6455 - val_loss: 0.7377 - val_accuracy: 0.5125\n",
      "Epoch 14/50\n",
      "880/880 [==============================] - 0s 286us/step - loss: 0.6344 - accuracy: 0.6455 - val_loss: 0.7414 - val_accuracy: 0.5125\n",
      "Epoch 15/50\n",
      "880/880 [==============================] - 0s 286us/step - loss: 0.6286 - accuracy: 0.6534 - val_loss: 0.7594 - val_accuracy: 0.5125\n",
      "Epoch 16/50\n",
      "880/880 [==============================] - 0s 290us/step - loss: 0.6208 - accuracy: 0.6670 - val_loss: 0.7649 - val_accuracy: 0.5000\n",
      "Epoch 17/50\n",
      "880/880 [==============================] - 0s 288us/step - loss: 0.6212 - accuracy: 0.6670 - val_loss: 0.7599 - val_accuracy: 0.5125\n",
      "Epoch 18/50\n",
      "880/880 [==============================] - 0s 285us/step - loss: 0.6095 - accuracy: 0.6489 - val_loss: 0.7871 - val_accuracy: 0.4750\n",
      "Epoch 19/50\n",
      "880/880 [==============================] - 0s 283us/step - loss: 0.6098 - accuracy: 0.6614 - val_loss: 0.7930 - val_accuracy: 0.4875\n",
      "Epoch 20/50\n",
      "880/880 [==============================] - 0s 286us/step - loss: 0.5989 - accuracy: 0.6909 - val_loss: 0.8114 - val_accuracy: 0.5000\n",
      "Epoch 21/50\n",
      "880/880 [==============================] - 0s 285us/step - loss: 0.5997 - accuracy: 0.6807 - val_loss: 0.8200 - val_accuracy: 0.4625\n",
      "Epoch 22/50\n",
      "880/880 [==============================] - 0s 286us/step - loss: 0.5855 - accuracy: 0.6795 - val_loss: 0.8503 - val_accuracy: 0.5000\n",
      "Epoch 23/50\n",
      "880/880 [==============================] - 0s 287us/step - loss: 0.5875 - accuracy: 0.6693 - val_loss: 0.8293 - val_accuracy: 0.4500\n",
      "Epoch 24/50\n",
      "880/880 [==============================] - 0s 283us/step - loss: 0.5785 - accuracy: 0.6852 - val_loss: 0.8462 - val_accuracy: 0.4750\n",
      "Epoch 25/50\n",
      "880/880 [==============================] - 0s 285us/step - loss: 0.5615 - accuracy: 0.6955 - val_loss: 0.9191 - val_accuracy: 0.4750\n",
      "Epoch 26/50\n",
      "880/880 [==============================] - 0s 285us/step - loss: 0.5523 - accuracy: 0.6955 - val_loss: 0.8792 - val_accuracy: 0.4625\n",
      "Epoch 27/50\n",
      "880/880 [==============================] - 0s 285us/step - loss: 0.5478 - accuracy: 0.7068 - val_loss: 0.8882 - val_accuracy: 0.5250\n",
      "Epoch 28/50\n",
      "880/880 [==============================] - 0s 287us/step - loss: 0.5417 - accuracy: 0.7080 - val_loss: 0.9608 - val_accuracy: 0.5125\n",
      "Epoch 29/50\n",
      "880/880 [==============================] - 0s 287us/step - loss: 0.5380 - accuracy: 0.6932 - val_loss: 0.9221 - val_accuracy: 0.5375\n",
      "Epoch 30/50\n",
      "880/880 [==============================] - 0s 287us/step - loss: 0.5384 - accuracy: 0.7000 - val_loss: 0.9175 - val_accuracy: 0.4750\n",
      "Epoch 31/50\n",
      "880/880 [==============================] - 0s 285us/step - loss: 0.5250 - accuracy: 0.7091 - val_loss: 0.9829 - val_accuracy: 0.5000\n",
      "Epoch 32/50\n",
      "880/880 [==============================] - 0s 286us/step - loss: 0.5151 - accuracy: 0.7193 - val_loss: 0.9805 - val_accuracy: 0.5125\n",
      "Epoch 33/50\n",
      "880/880 [==============================] - 0s 285us/step - loss: 0.5256 - accuracy: 0.7125 - val_loss: 0.9926 - val_accuracy: 0.5250\n",
      "Epoch 34/50\n",
      "880/880 [==============================] - 0s 287us/step - loss: 0.5147 - accuracy: 0.7295 - val_loss: 1.0272 - val_accuracy: 0.5500\n",
      "Epoch 35/50\n",
      "880/880 [==============================] - 0s 285us/step - loss: 0.5244 - accuracy: 0.7159 - val_loss: 0.9790 - val_accuracy: 0.4625\n",
      "Epoch 36/50\n",
      "880/880 [==============================] - 0s 287us/step - loss: 0.5023 - accuracy: 0.7239 - val_loss: 1.0288 - val_accuracy: 0.5125\n",
      "Epoch 37/50\n",
      "880/880 [==============================] - 0s 285us/step - loss: 0.5075 - accuracy: 0.7182 - val_loss: 1.0214 - val_accuracy: 0.5125\n",
      "Epoch 38/50\n",
      "880/880 [==============================] - 0s 287us/step - loss: 0.5124 - accuracy: 0.7205 - val_loss: 0.9902 - val_accuracy: 0.4625\n",
      "Epoch 39/50\n",
      "880/880 [==============================] - 0s 287us/step - loss: 0.4995 - accuracy: 0.7148 - val_loss: 1.0972 - val_accuracy: 0.5125\n",
      "Epoch 40/50\n",
      "880/880 [==============================] - 0s 286us/step - loss: 0.4936 - accuracy: 0.7273 - val_loss: 1.0670 - val_accuracy: 0.4625\n",
      "Epoch 41/50\n",
      "880/880 [==============================] - 0s 285us/step - loss: 0.4967 - accuracy: 0.7148 - val_loss: 1.1230 - val_accuracy: 0.5375\n",
      "Epoch 42/50\n",
      "880/880 [==============================] - 0s 285us/step - loss: 0.4894 - accuracy: 0.7273 - val_loss: 1.0558 - val_accuracy: 0.4875\n",
      "Epoch 43/50\n",
      "880/880 [==============================] - 0s 286us/step - loss: 0.4973 - accuracy: 0.7307 - val_loss: 1.1027 - val_accuracy: 0.4750\n",
      "Epoch 44/50\n",
      "880/880 [==============================] - 0s 287us/step - loss: 0.4977 - accuracy: 0.7182 - val_loss: 1.1399 - val_accuracy: 0.5000\n",
      "Epoch 45/50\n",
      "880/880 [==============================] - 0s 286us/step - loss: 0.4897 - accuracy: 0.7284 - val_loss: 1.1017 - val_accuracy: 0.5000\n",
      "Epoch 46/50\n",
      "880/880 [==============================] - 0s 288us/step - loss: 0.4891 - accuracy: 0.7398 - val_loss: 1.1910 - val_accuracy: 0.5250\n",
      "Epoch 47/50\n",
      "880/880 [==============================] - 0s 285us/step - loss: 0.4819 - accuracy: 0.7398 - val_loss: 1.1224 - val_accuracy: 0.5375\n",
      "Epoch 48/50\n",
      "880/880 [==============================] - 0s 284us/step - loss: 0.4794 - accuracy: 0.7409 - val_loss: 1.2138 - val_accuracy: 0.5125\n",
      "Epoch 49/50\n",
      "880/880 [==============================] - 0s 285us/step - loss: 0.4841 - accuracy: 0.7318 - val_loss: 1.1507 - val_accuracy: 0.5500\n",
      "Epoch 50/50\n",
      "880/880 [==============================] - 0s 286us/step - loss: 0.4797 - accuracy: 0.7352 - val_loss: 1.1984 - val_accuracy: 0.5250\n",
      "Score for fold 5: loss of 1.1983515739440918; accuracy of 52.49999761581421%\n",
      "Model: \"sequential_138\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_686 (Dense)            (None, 100)               2500      \n",
      "_________________________________________________________________\n",
      "dense_687 (Dense)            (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "dense_688 (Dense)            (None, 25)                1275      \n",
      "_________________________________________________________________\n",
      "dense_689 (Dense)            (None, 10)                260       \n",
      "_________________________________________________________________\n",
      "dense_690 (Dense)            (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 9,096\n",
      "Trainable params: 9,096\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 6 ...\n",
      "Train on 880 samples, validate on 80 samples\n",
      "Epoch 1/50\n",
      "880/880 [==============================] - 3s 4ms/step - loss: 0.7018 - accuracy: 0.4761 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 2/50\n",
      "880/880 [==============================] - 0s 288us/step - loss: 0.6902 - accuracy: 0.5443 - val_loss: 0.6918 - val_accuracy: 0.5125\n",
      "Epoch 3/50\n",
      "880/880 [==============================] - 0s 289us/step - loss: 0.6889 - accuracy: 0.5455 - val_loss: 0.6913 - val_accuracy: 0.5250\n",
      "Epoch 4/50\n",
      "880/880 [==============================] - 0s 290us/step - loss: 0.6881 - accuracy: 0.5443 - val_loss: 0.6878 - val_accuracy: 0.5750\n",
      "Epoch 5/50\n",
      "880/880 [==============================] - 0s 283us/step - loss: 0.6863 - accuracy: 0.5545 - val_loss: 0.6898 - val_accuracy: 0.5250\n",
      "Epoch 6/50\n",
      "880/880 [==============================] - 0s 285us/step - loss: 0.6823 - accuracy: 0.5761 - val_loss: 0.6891 - val_accuracy: 0.5250\n",
      "Epoch 7/50\n",
      "880/880 [==============================] - 0s 282us/step - loss: 0.6775 - accuracy: 0.5989 - val_loss: 0.6868 - val_accuracy: 0.5625\n",
      "Epoch 8/50\n",
      "880/880 [==============================] - 0s 286us/step - loss: 0.6749 - accuracy: 0.5852 - val_loss: 0.6929 - val_accuracy: 0.5375\n",
      "Epoch 9/50\n",
      "880/880 [==============================] - 0s 284us/step - loss: 0.6707 - accuracy: 0.6114 - val_loss: 0.6900 - val_accuracy: 0.5500\n",
      "Epoch 10/50\n",
      "880/880 [==============================] - 0s 286us/step - loss: 0.6641 - accuracy: 0.6114 - val_loss: 0.6880 - val_accuracy: 0.5625\n",
      "Epoch 11/50\n",
      "880/880 [==============================] - 0s 282us/step - loss: 0.6604 - accuracy: 0.6227 - val_loss: 0.6939 - val_accuracy: 0.5500\n",
      "Epoch 12/50\n",
      "880/880 [==============================] - 0s 286us/step - loss: 0.6550 - accuracy: 0.6307 - val_loss: 0.6996 - val_accuracy: 0.5250\n",
      "Epoch 13/50\n",
      "880/880 [==============================] - 0s 284us/step - loss: 0.6686 - accuracy: 0.5818 - val_loss: 0.6905 - val_accuracy: 0.5500\n",
      "Epoch 14/50\n",
      "880/880 [==============================] - 0s 288us/step - loss: 0.6544 - accuracy: 0.6386 - val_loss: 0.7037 - val_accuracy: 0.5250\n",
      "Epoch 15/50\n",
      "880/880 [==============================] - 0s 286us/step - loss: 0.6419 - accuracy: 0.6477 - val_loss: 0.7048 - val_accuracy: 0.5625\n",
      "Epoch 16/50\n",
      "880/880 [==============================] - 0s 290us/step - loss: 0.6330 - accuracy: 0.6523 - val_loss: 0.7172 - val_accuracy: 0.5375\n",
      "Epoch 17/50\n",
      "880/880 [==============================] - 0s 285us/step - loss: 0.6248 - accuracy: 0.6580 - val_loss: 0.7250 - val_accuracy: 0.5500\n",
      "Epoch 18/50\n",
      "880/880 [==============================] - 0s 290us/step - loss: 0.6282 - accuracy: 0.6409 - val_loss: 0.7076 - val_accuracy: 0.5500\n",
      "Epoch 19/50\n",
      "880/880 [==============================] - 0s 286us/step - loss: 0.6336 - accuracy: 0.6307 - val_loss: 0.7215 - val_accuracy: 0.5375\n",
      "Epoch 20/50\n",
      "880/880 [==============================] - 0s 288us/step - loss: 0.6043 - accuracy: 0.6693 - val_loss: 0.7482 - val_accuracy: 0.5375\n",
      "Epoch 21/50\n",
      "880/880 [==============================] - 0s 288us/step - loss: 0.5913 - accuracy: 0.6818 - val_loss: 0.7782 - val_accuracy: 0.5375\n",
      "Epoch 22/50\n",
      "880/880 [==============================] - 0s 288us/step - loss: 0.5783 - accuracy: 0.6898 - val_loss: 0.7779 - val_accuracy: 0.5125\n",
      "Epoch 23/50\n",
      "880/880 [==============================] - 0s 286us/step - loss: 0.5829 - accuracy: 0.6932 - val_loss: 0.7970 - val_accuracy: 0.5125\n",
      "Epoch 24/50\n",
      "880/880 [==============================] - 0s 287us/step - loss: 0.5747 - accuracy: 0.6977 - val_loss: 0.8483 - val_accuracy: 0.5375\n",
      "Epoch 25/50\n",
      "880/880 [==============================] - 0s 287us/step - loss: 0.5657 - accuracy: 0.6989 - val_loss: 0.7932 - val_accuracy: 0.5000\n",
      "Epoch 26/50\n",
      "880/880 [==============================] - 0s 287us/step - loss: 0.5544 - accuracy: 0.6955 - val_loss: 0.8562 - val_accuracy: 0.5250\n",
      "Epoch 27/50\n",
      "880/880 [==============================] - 0s 285us/step - loss: 0.5556 - accuracy: 0.7136 - val_loss: 0.8544 - val_accuracy: 0.5250\n",
      "Epoch 28/50\n",
      "880/880 [==============================] - 0s 295us/step - loss: 0.5558 - accuracy: 0.7114 - val_loss: 0.8370 - val_accuracy: 0.4625\n",
      "Epoch 29/50\n",
      "880/880 [==============================] - 0s 285us/step - loss: 0.5422 - accuracy: 0.7057 - val_loss: 0.9238 - val_accuracy: 0.5375\n",
      "Epoch 30/50\n",
      "880/880 [==============================] - 0s 287us/step - loss: 0.5282 - accuracy: 0.7136 - val_loss: 0.9305 - val_accuracy: 0.5125\n",
      "Epoch 31/50\n",
      "880/880 [==============================] - 0s 284us/step - loss: 0.5346 - accuracy: 0.7136 - val_loss: 0.9884 - val_accuracy: 0.5000\n",
      "Epoch 32/50\n",
      "880/880 [==============================] - 0s 287us/step - loss: 0.5316 - accuracy: 0.7114 - val_loss: 0.9817 - val_accuracy: 0.5250\n",
      "Epoch 33/50\n",
      "880/880 [==============================] - 0s 288us/step - loss: 0.5137 - accuracy: 0.7261 - val_loss: 0.9383 - val_accuracy: 0.5125\n",
      "Epoch 34/50\n",
      "880/880 [==============================] - 0s 288us/step - loss: 0.5137 - accuracy: 0.7182 - val_loss: 0.9563 - val_accuracy: 0.5125\n",
      "Epoch 35/50\n",
      "880/880 [==============================] - 0s 290us/step - loss: 0.5089 - accuracy: 0.7352 - val_loss: 0.9892 - val_accuracy: 0.5125\n",
      "Epoch 36/50\n",
      "880/880 [==============================] - 0s 288us/step - loss: 0.5098 - accuracy: 0.7284 - val_loss: 0.9819 - val_accuracy: 0.5375\n",
      "Epoch 37/50\n",
      "880/880 [==============================] - 0s 290us/step - loss: 0.4990 - accuracy: 0.7307 - val_loss: 1.0095 - val_accuracy: 0.5000\n",
      "Epoch 38/50\n",
      "880/880 [==============================] - 0s 286us/step - loss: 0.4962 - accuracy: 0.7341 - val_loss: 1.0494 - val_accuracy: 0.5250\n",
      "Epoch 39/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "880/880 [==============================] - 0s 288us/step - loss: 0.4988 - accuracy: 0.7148 - val_loss: 1.0174 - val_accuracy: 0.5000\n",
      "Epoch 40/50\n",
      "880/880 [==============================] - 0s 286us/step - loss: 0.4971 - accuracy: 0.7352 - val_loss: 0.9848 - val_accuracy: 0.5250\n",
      "Epoch 41/50\n",
      "880/880 [==============================] - 0s 287us/step - loss: 0.5003 - accuracy: 0.7250 - val_loss: 1.0474 - val_accuracy: 0.4875\n",
      "Epoch 42/50\n",
      "880/880 [==============================] - 0s 288us/step - loss: 0.5036 - accuracy: 0.7205 - val_loss: 1.0840 - val_accuracy: 0.5125\n",
      "Epoch 43/50\n",
      "880/880 [==============================] - 0s 286us/step - loss: 0.4895 - accuracy: 0.7295 - val_loss: 1.0765 - val_accuracy: 0.5375\n",
      "Epoch 44/50\n",
      "880/880 [==============================] - 0s 287us/step - loss: 0.4871 - accuracy: 0.7307 - val_loss: 1.0348 - val_accuracy: 0.5125\n",
      "Epoch 45/50\n",
      "880/880 [==============================] - 0s 288us/step - loss: 0.4819 - accuracy: 0.7375 - val_loss: 1.0698 - val_accuracy: 0.5500\n",
      "Epoch 46/50\n",
      "880/880 [==============================] - 0s 286us/step - loss: 0.4783 - accuracy: 0.7409 - val_loss: 1.0943 - val_accuracy: 0.5375\n",
      "Epoch 47/50\n",
      "880/880 [==============================] - 0s 288us/step - loss: 0.4828 - accuracy: 0.7250 - val_loss: 1.1320 - val_accuracy: 0.5375\n",
      "Epoch 48/50\n",
      "880/880 [==============================] - 0s 290us/step - loss: 0.4882 - accuracy: 0.7364 - val_loss: 1.0667 - val_accuracy: 0.5375\n",
      "Epoch 49/50\n",
      "880/880 [==============================] - 0s 285us/step - loss: 0.4845 - accuracy: 0.7295 - val_loss: 1.1091 - val_accuracy: 0.5500\n",
      "Epoch 50/50\n",
      "880/880 [==============================] - 0s 284us/step - loss: 0.4789 - accuracy: 0.7273 - val_loss: 1.0355 - val_accuracy: 0.5375\n",
      "Score for fold 6: loss of 1.0355484008789062; accuracy of 53.75000238418579%\n",
      "Model: \"sequential_139\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_691 (Dense)            (None, 100)               2500      \n",
      "_________________________________________________________________\n",
      "dense_692 (Dense)            (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "dense_693 (Dense)            (None, 25)                1275      \n",
      "_________________________________________________________________\n",
      "dense_694 (Dense)            (None, 10)                260       \n",
      "_________________________________________________________________\n",
      "dense_695 (Dense)            (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 9,096\n",
      "Trainable params: 9,096\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 7 ...\n",
      "Train on 880 samples, validate on 80 samples\n",
      "Epoch 1/50\n",
      "880/880 [==============================] - 3s 4ms/step - loss: 0.6998 - accuracy: 0.5375 - val_loss: 0.6978 - val_accuracy: 0.4875\n",
      "Epoch 2/50\n",
      "880/880 [==============================] - 0s 287us/step - loss: 0.6891 - accuracy: 0.5375 - val_loss: 0.7026 - val_accuracy: 0.5000\n",
      "Epoch 3/50\n",
      "880/880 [==============================] - 0s 288us/step - loss: 0.6856 - accuracy: 0.5375 - val_loss: 0.6958 - val_accuracy: 0.5000\n",
      "Epoch 4/50\n",
      "880/880 [==============================] - 0s 297us/step - loss: 0.6825 - accuracy: 0.5375 - val_loss: 0.6951 - val_accuracy: 0.4750\n",
      "Epoch 5/50\n",
      "880/880 [==============================] - 0s 288us/step - loss: 0.6826 - accuracy: 0.5375 - val_loss: 0.7002 - val_accuracy: 0.5000\n",
      "Epoch 6/50\n",
      "880/880 [==============================] - 0s 289us/step - loss: 0.6763 - accuracy: 0.5375 - val_loss: 0.7019 - val_accuracy: 0.5250\n",
      "Epoch 7/50\n",
      "880/880 [==============================] - 0s 289us/step - loss: 0.6723 - accuracy: 0.5682 - val_loss: 0.7140 - val_accuracy: 0.4875\n",
      "Epoch 8/50\n",
      "880/880 [==============================] - 0s 288us/step - loss: 0.6699 - accuracy: 0.5682 - val_loss: 0.7190 - val_accuracy: 0.5625\n",
      "Epoch 9/50\n",
      "880/880 [==============================] - 0s 289us/step - loss: 0.6661 - accuracy: 0.5830 - val_loss: 0.7190 - val_accuracy: 0.5500\n",
      "Epoch 10/50\n",
      "880/880 [==============================] - 0s 289us/step - loss: 0.6607 - accuracy: 0.5932 - val_loss: 0.7261 - val_accuracy: 0.5500\n",
      "Epoch 11/50\n",
      "880/880 [==============================] - 0s 295us/step - loss: 0.6563 - accuracy: 0.6080 - val_loss: 0.7369 - val_accuracy: 0.5000\n",
      "Epoch 12/50\n",
      "880/880 [==============================] - 0s 291us/step - loss: 0.6497 - accuracy: 0.6125 - val_loss: 0.7643 - val_accuracy: 0.5125\n",
      "Epoch 13/50\n",
      "880/880 [==============================] - 0s 287us/step - loss: 0.6462 - accuracy: 0.6148 - val_loss: 0.7526 - val_accuracy: 0.4625\n",
      "Epoch 14/50\n",
      "880/880 [==============================] - 0s 292us/step - loss: 0.6390 - accuracy: 0.6409 - val_loss: 0.8072 - val_accuracy: 0.5375\n",
      "Epoch 15/50\n",
      "880/880 [==============================] - 0s 290us/step - loss: 0.6383 - accuracy: 0.6216 - val_loss: 0.7662 - val_accuracy: 0.5000\n",
      "Epoch 16/50\n",
      "880/880 [==============================] - 0s 290us/step - loss: 0.6261 - accuracy: 0.6477 - val_loss: 0.7955 - val_accuracy: 0.4875\n",
      "Epoch 17/50\n",
      "880/880 [==============================] - 0s 289us/step - loss: 0.6158 - accuracy: 0.6443 - val_loss: 0.8551 - val_accuracy: 0.5000\n",
      "Epoch 18/50\n",
      "880/880 [==============================] - 0s 291us/step - loss: 0.6048 - accuracy: 0.6466 - val_loss: 0.9424 - val_accuracy: 0.4875\n",
      "Epoch 19/50\n",
      "880/880 [==============================] - 0s 288us/step - loss: 0.6083 - accuracy: 0.6477 - val_loss: 0.8569 - val_accuracy: 0.5000\n",
      "Epoch 20/50\n",
      "880/880 [==============================] - 0s 288us/step - loss: 0.6080 - accuracy: 0.6466 - val_loss: 0.8705 - val_accuracy: 0.5250\n",
      "Epoch 21/50\n",
      "880/880 [==============================] - 0s 288us/step - loss: 0.5879 - accuracy: 0.6727 - val_loss: 0.8772 - val_accuracy: 0.5250\n",
      "Epoch 22/50\n",
      "880/880 [==============================] - 0s 290us/step - loss: 0.5851 - accuracy: 0.6807 - val_loss: 0.9353 - val_accuracy: 0.4625\n",
      "Epoch 23/50\n",
      "880/880 [==============================] - 0s 292us/step - loss: 0.5773 - accuracy: 0.6750 - val_loss: 0.9428 - val_accuracy: 0.5000\n",
      "Epoch 24/50\n",
      "880/880 [==============================] - 0s 289us/step - loss: 0.5724 - accuracy: 0.6818 - val_loss: 0.9215 - val_accuracy: 0.5000\n",
      "Epoch 25/50\n",
      "880/880 [==============================] - 0s 288us/step - loss: 0.5564 - accuracy: 0.6784 - val_loss: 1.0534 - val_accuracy: 0.4625\n",
      "Epoch 26/50\n",
      "880/880 [==============================] - 0s 289us/step - loss: 0.5804 - accuracy: 0.6716 - val_loss: 1.0695 - val_accuracy: 0.5000\n",
      "Epoch 27/50\n",
      "880/880 [==============================] - 0s 290us/step - loss: 0.5823 - accuracy: 0.6580 - val_loss: 0.9451 - val_accuracy: 0.4625\n",
      "Epoch 28/50\n",
      "880/880 [==============================] - 0s 291us/step - loss: 0.5653 - accuracy: 0.6761 - val_loss: 0.9987 - val_accuracy: 0.4750\n",
      "Epoch 29/50\n",
      "880/880 [==============================] - 0s 287us/step - loss: 0.5576 - accuracy: 0.6898 - val_loss: 0.9566 - val_accuracy: 0.5125\n",
      "Epoch 30/50\n",
      "880/880 [==============================] - 0s 291us/step - loss: 0.5565 - accuracy: 0.6886 - val_loss: 1.0575 - val_accuracy: 0.5000\n",
      "Epoch 31/50\n",
      "880/880 [==============================] - 0s 291us/step - loss: 0.5368 - accuracy: 0.7125 - val_loss: 1.0623 - val_accuracy: 0.4875\n",
      "Epoch 32/50\n",
      "880/880 [==============================] - 0s 289us/step - loss: 0.5327 - accuracy: 0.7011 - val_loss: 1.0669 - val_accuracy: 0.4625\n",
      "Epoch 33/50\n",
      "880/880 [==============================] - 0s 293us/step - loss: 0.5336 - accuracy: 0.7057 - val_loss: 1.1271 - val_accuracy: 0.4875\n",
      "Epoch 34/50\n",
      "880/880 [==============================] - 0s 290us/step - loss: 0.5298 - accuracy: 0.7045 - val_loss: 1.1617 - val_accuracy: 0.5125\n",
      "Epoch 35/50\n",
      "880/880 [==============================] - 0s 289us/step - loss: 0.5177 - accuracy: 0.7193 - val_loss: 1.1714 - val_accuracy: 0.4750\n",
      "Epoch 36/50\n",
      "880/880 [==============================] - 0s 286us/step - loss: 0.5291 - accuracy: 0.7080 - val_loss: 1.1513 - val_accuracy: 0.5250\n",
      "Epoch 37/50\n",
      "880/880 [==============================] - 0s 290us/step - loss: 0.5127 - accuracy: 0.7205 - val_loss: 1.1646 - val_accuracy: 0.4625\n",
      "Epoch 38/50\n",
      "880/880 [==============================] - 0s 295us/step - loss: 0.5084 - accuracy: 0.7341 - val_loss: 1.2318 - val_accuracy: 0.5250\n",
      "Epoch 39/50\n",
      "880/880 [==============================] - 0s 289us/step - loss: 0.5073 - accuracy: 0.7307 - val_loss: 1.2050 - val_accuracy: 0.5375\n",
      "Epoch 40/50\n",
      "880/880 [==============================] - 0s 290us/step - loss: 0.5094 - accuracy: 0.7216 - val_loss: 1.2772 - val_accuracy: 0.5000\n",
      "Epoch 41/50\n",
      "880/880 [==============================] - 0s 288us/step - loss: 0.5175 - accuracy: 0.7159 - val_loss: 1.1191 - val_accuracy: 0.5625\n",
      "Epoch 42/50\n",
      "880/880 [==============================] - 0s 286us/step - loss: 0.5178 - accuracy: 0.7023 - val_loss: 1.1828 - val_accuracy: 0.5000\n",
      "Epoch 43/50\n",
      "880/880 [==============================] - 0s 289us/step - loss: 0.4994 - accuracy: 0.7284 - val_loss: 1.3018 - val_accuracy: 0.5000\n",
      "Epoch 44/50\n",
      "880/880 [==============================] - 0s 289us/step - loss: 0.5017 - accuracy: 0.7239 - val_loss: 1.2342 - val_accuracy: 0.5250\n",
      "Epoch 45/50\n",
      "880/880 [==============================] - 0s 291us/step - loss: 0.4974 - accuracy: 0.7250 - val_loss: 1.2585 - val_accuracy: 0.5250\n",
      "Epoch 46/50\n",
      "880/880 [==============================] - 0s 291us/step - loss: 0.4918 - accuracy: 0.7307 - val_loss: 1.2831 - val_accuracy: 0.5250\n",
      "Epoch 47/50\n",
      "880/880 [==============================] - 0s 290us/step - loss: 0.4884 - accuracy: 0.7375 - val_loss: 1.3466 - val_accuracy: 0.5375\n",
      "Epoch 48/50\n",
      "880/880 [==============================] - 0s 289us/step - loss: 0.4894 - accuracy: 0.7330 - val_loss: 1.3756 - val_accuracy: 0.5250\n",
      "Epoch 49/50\n",
      "880/880 [==============================] - 0s 287us/step - loss: 0.4948 - accuracy: 0.7227 - val_loss: 1.2476 - val_accuracy: 0.5000\n",
      "Epoch 50/50\n",
      "880/880 [==============================] - 0s 291us/step - loss: 0.4907 - accuracy: 0.7409 - val_loss: 1.3397 - val_accuracy: 0.5250\n",
      "Score for fold 7: loss of 1.3397101163864136; accuracy of 52.49999761581421%\n",
      "Model: \"sequential_140\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_696 (Dense)            (None, 100)               2500      \n",
      "_________________________________________________________________\n",
      "dense_697 (Dense)            (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "dense_698 (Dense)            (None, 25)                1275      \n",
      "_________________________________________________________________\n",
      "dense_699 (Dense)            (None, 10)                260       \n",
      "_________________________________________________________________\n",
      "dense_700 (Dense)            (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 9,096\n",
      "Trainable params: 9,096\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 8 ...\n",
      "Train on 880 samples, validate on 80 samples\n",
      "Epoch 1/50\n",
      "880/880 [==============================] - 3s 4ms/step - loss: 0.6927 - accuracy: 0.5375 - val_loss: 0.6930 - val_accuracy: 0.5000\n",
      "Epoch 2/50\n",
      "880/880 [==============================] - 0s 298us/step - loss: 0.6888 - accuracy: 0.5375 - val_loss: 0.6941 - val_accuracy: 0.5000\n",
      "Epoch 3/50\n",
      "880/880 [==============================] - 0s 289us/step - loss: 0.6867 - accuracy: 0.5375 - val_loss: 0.6945 - val_accuracy: 0.5000\n",
      "Epoch 4/50\n",
      "880/880 [==============================] - 0s 304us/step - loss: 0.6874 - accuracy: 0.5398 - val_loss: 0.6959 - val_accuracy: 0.5250\n",
      "Epoch 5/50\n",
      "880/880 [==============================] - 0s 292us/step - loss: 0.6830 - accuracy: 0.5398 - val_loss: 0.6947 - val_accuracy: 0.4875\n",
      "Epoch 6/50\n",
      "880/880 [==============================] - 0s 292us/step - loss: 0.6808 - accuracy: 0.5545 - val_loss: 0.6995 - val_accuracy: 0.5000\n",
      "Epoch 7/50\n",
      "880/880 [==============================] - 0s 291us/step - loss: 0.6774 - accuracy: 0.5602 - val_loss: 0.7028 - val_accuracy: 0.5000\n",
      "Epoch 8/50\n",
      "880/880 [==============================] - 0s 288us/step - loss: 0.6725 - accuracy: 0.5898 - val_loss: 0.7107 - val_accuracy: 0.5375\n",
      "Epoch 9/50\n",
      "880/880 [==============================] - 0s 287us/step - loss: 0.6729 - accuracy: 0.5966 - val_loss: 0.7180 - val_accuracy: 0.5250\n",
      "Epoch 10/50\n",
      "880/880 [==============================] - 0s 291us/step - loss: 0.6639 - accuracy: 0.6045 - val_loss: 0.7319 - val_accuracy: 0.5375\n",
      "Epoch 11/50\n",
      "880/880 [==============================] - 0s 293us/step - loss: 0.6607 - accuracy: 0.5795 - val_loss: 0.7190 - val_accuracy: 0.4875\n",
      "Epoch 12/50\n",
      "880/880 [==============================] - 0s 291us/step - loss: 0.6576 - accuracy: 0.5943 - val_loss: 0.7587 - val_accuracy: 0.5625\n",
      "Epoch 13/50\n",
      "880/880 [==============================] - 0s 290us/step - loss: 0.6501 - accuracy: 0.6080 - val_loss: 0.7918 - val_accuracy: 0.5375\n",
      "Epoch 14/50\n",
      "880/880 [==============================] - 0s 290us/step - loss: 0.6465 - accuracy: 0.6102 - val_loss: 0.7901 - val_accuracy: 0.4625\n",
      "Epoch 15/50\n",
      "880/880 [==============================] - 0s 286us/step - loss: 0.6488 - accuracy: 0.6045 - val_loss: 0.8269 - val_accuracy: 0.5375\n",
      "Epoch 16/50\n",
      "880/880 [==============================] - 0s 290us/step - loss: 0.6346 - accuracy: 0.6239 - val_loss: 0.8214 - val_accuracy: 0.5125\n",
      "Epoch 17/50\n",
      "880/880 [==============================] - 0s 289us/step - loss: 0.6272 - accuracy: 0.6443 - val_loss: 0.8737 - val_accuracy: 0.5000\n",
      "Epoch 18/50\n",
      "880/880 [==============================] - 0s 291us/step - loss: 0.6216 - accuracy: 0.6375 - val_loss: 0.9173 - val_accuracy: 0.4875\n",
      "Epoch 19/50\n",
      "880/880 [==============================] - 0s 288us/step - loss: 0.6165 - accuracy: 0.6534 - val_loss: 0.8742 - val_accuracy: 0.4750\n",
      "Epoch 20/50\n",
      "880/880 [==============================] - 0s 297us/step - loss: 0.6196 - accuracy: 0.6443 - val_loss: 0.8554 - val_accuracy: 0.4750\n",
      "Epoch 21/50\n",
      "880/880 [==============================] - 0s 289us/step - loss: 0.6148 - accuracy: 0.6477 - val_loss: 1.0133 - val_accuracy: 0.4500\n",
      "Epoch 22/50\n",
      "880/880 [==============================] - 0s 290us/step - loss: 0.6107 - accuracy: 0.6670 - val_loss: 0.9356 - val_accuracy: 0.4750\n",
      "Epoch 23/50\n",
      "880/880 [==============================] - 0s 290us/step - loss: 0.6011 - accuracy: 0.6784 - val_loss: 0.9847 - val_accuracy: 0.4625\n",
      "Epoch 24/50\n",
      "880/880 [==============================] - 0s 289us/step - loss: 0.5943 - accuracy: 0.6534 - val_loss: 0.9537 - val_accuracy: 0.4250\n",
      "Epoch 25/50\n",
      "880/880 [==============================] - 0s 291us/step - loss: 0.5861 - accuracy: 0.6818 - val_loss: 0.9710 - val_accuracy: 0.4875\n",
      "Epoch 26/50\n",
      "880/880 [==============================] - 0s 290us/step - loss: 0.5804 - accuracy: 0.6898 - val_loss: 1.0023 - val_accuracy: 0.4750\n",
      "Epoch 27/50\n",
      "880/880 [==============================] - 0s 292us/step - loss: 0.5726 - accuracy: 0.6898 - val_loss: 0.9937 - val_accuracy: 0.4625\n",
      "Epoch 28/50\n",
      "880/880 [==============================] - 0s 291us/step - loss: 0.5701 - accuracy: 0.6841 - val_loss: 1.0094 - val_accuracy: 0.4375\n",
      "Epoch 29/50\n",
      "880/880 [==============================] - 0s 288us/step - loss: 0.5665 - accuracy: 0.6875 - val_loss: 1.0865 - val_accuracy: 0.4500\n",
      "Epoch 30/50\n",
      "880/880 [==============================] - 0s 298us/step - loss: 0.5534 - accuracy: 0.6977 - val_loss: 1.1650 - val_accuracy: 0.4625\n",
      "Epoch 31/50\n",
      "880/880 [==============================] - 0s 290us/step - loss: 0.5457 - accuracy: 0.6989 - val_loss: 1.0732 - val_accuracy: 0.4875\n",
      "Epoch 32/50\n",
      "880/880 [==============================] - 0s 290us/step - loss: 0.5587 - accuracy: 0.6932 - val_loss: 1.2676 - val_accuracy: 0.4750\n",
      "Epoch 33/50\n",
      "880/880 [==============================] - 0s 287us/step - loss: 0.5495 - accuracy: 0.7091 - val_loss: 1.1158 - val_accuracy: 0.4875\n",
      "Epoch 34/50\n",
      "880/880 [==============================] - 0s 286us/step - loss: 0.5367 - accuracy: 0.7023 - val_loss: 1.1622 - val_accuracy: 0.4750\n",
      "Epoch 35/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "880/880 [==============================] - 0s 287us/step - loss: 0.5362 - accuracy: 0.7091 - val_loss: 1.0982 - val_accuracy: 0.4625\n",
      "Epoch 36/50\n",
      "880/880 [==============================] - 0s 291us/step - loss: 0.5251 - accuracy: 0.7250 - val_loss: 1.3180 - val_accuracy: 0.4875\n",
      "Epoch 37/50\n",
      "880/880 [==============================] - 0s 287us/step - loss: 0.5234 - accuracy: 0.7261 - val_loss: 1.2105 - val_accuracy: 0.4875\n",
      "Epoch 38/50\n",
      "880/880 [==============================] - 0s 288us/step - loss: 0.5240 - accuracy: 0.7125 - val_loss: 1.1856 - val_accuracy: 0.4625\n",
      "Epoch 39/50\n",
      "880/880 [==============================] - 0s 289us/step - loss: 0.5214 - accuracy: 0.7205 - val_loss: 1.2898 - val_accuracy: 0.5000\n",
      "Epoch 40/50\n",
      "880/880 [==============================] - 0s 296us/step - loss: 0.5322 - accuracy: 0.7011 - val_loss: 1.5693 - val_accuracy: 0.5375\n",
      "Epoch 41/50\n",
      "880/880 [==============================] - 0s 290us/step - loss: 0.5226 - accuracy: 0.7205 - val_loss: 1.3349 - val_accuracy: 0.4875\n",
      "Epoch 42/50\n",
      "880/880 [==============================] - 0s 293us/step - loss: 0.5092 - accuracy: 0.7250 - val_loss: 1.4353 - val_accuracy: 0.4625\n",
      "Epoch 43/50\n",
      "880/880 [==============================] - 0s 288us/step - loss: 0.5103 - accuracy: 0.7261 - val_loss: 1.2821 - val_accuracy: 0.4750\n",
      "Epoch 44/50\n",
      "880/880 [==============================] - 0s 291us/step - loss: 0.5047 - accuracy: 0.7170 - val_loss: 1.3902 - val_accuracy: 0.5000\n",
      "Epoch 45/50\n",
      "880/880 [==============================] - 0s 291us/step - loss: 0.4962 - accuracy: 0.7250 - val_loss: 1.4098 - val_accuracy: 0.4875\n",
      "Epoch 46/50\n",
      "880/880 [==============================] - 0s 290us/step - loss: 0.4900 - accuracy: 0.7295 - val_loss: 1.4603 - val_accuracy: 0.4875\n",
      "Epoch 47/50\n",
      "880/880 [==============================] - 0s 290us/step - loss: 0.4907 - accuracy: 0.7443 - val_loss: 1.4067 - val_accuracy: 0.4750\n",
      "Epoch 48/50\n",
      "880/880 [==============================] - 0s 296us/step - loss: 0.4921 - accuracy: 0.7318 - val_loss: 1.5586 - val_accuracy: 0.4875\n",
      "Epoch 49/50\n",
      "880/880 [==============================] - 0s 287us/step - loss: 0.4925 - accuracy: 0.7307 - val_loss: 1.6940 - val_accuracy: 0.5125\n",
      "Epoch 50/50\n",
      "880/880 [==============================] - 0s 289us/step - loss: 0.4938 - accuracy: 0.7193 - val_loss: 1.6226 - val_accuracy: 0.5125\n",
      "Score for fold 8: loss of 1.6226293087005614; accuracy of 51.249998807907104%\n",
      "Model: \"sequential_141\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_701 (Dense)            (None, 100)               2500      \n",
      "_________________________________________________________________\n",
      "dense_702 (Dense)            (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "dense_703 (Dense)            (None, 25)                1275      \n",
      "_________________________________________________________________\n",
      "dense_704 (Dense)            (None, 10)                260       \n",
      "_________________________________________________________________\n",
      "dense_705 (Dense)            (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 9,096\n",
      "Trainable params: 9,096\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 9 ...\n",
      "Train on 880 samples, validate on 80 samples\n",
      "Epoch 1/50\n",
      "880/880 [==============================] - 3s 4ms/step - loss: 0.6964 - accuracy: 0.5102 - val_loss: 0.6911 - val_accuracy: 0.5250\n",
      "Epoch 2/50\n",
      "880/880 [==============================] - 0s 297us/step - loss: 0.6870 - accuracy: 0.5443 - val_loss: 0.6913 - val_accuracy: 0.5125\n",
      "Epoch 3/50\n",
      "880/880 [==============================] - 0s 299us/step - loss: 0.6841 - accuracy: 0.5705 - val_loss: 0.6920 - val_accuracy: 0.5875\n",
      "Epoch 4/50\n",
      "880/880 [==============================] - 0s 292us/step - loss: 0.6797 - accuracy: 0.5670 - val_loss: 0.6921 - val_accuracy: 0.5375\n",
      "Epoch 5/50\n",
      "880/880 [==============================] - 0s 291us/step - loss: 0.6758 - accuracy: 0.5864 - val_loss: 0.6961 - val_accuracy: 0.4875\n",
      "Epoch 6/50\n",
      "880/880 [==============================] - 0s 293us/step - loss: 0.6744 - accuracy: 0.5852 - val_loss: 0.6997 - val_accuracy: 0.5250\n",
      "Epoch 7/50\n",
      "880/880 [==============================] - 0s 293us/step - loss: 0.6702 - accuracy: 0.5875 - val_loss: 0.7013 - val_accuracy: 0.5250\n",
      "Epoch 8/50\n",
      "880/880 [==============================] - 0s 295us/step - loss: 0.6622 - accuracy: 0.6148 - val_loss: 0.7144 - val_accuracy: 0.5375\n",
      "Epoch 9/50\n",
      "880/880 [==============================] - 0s 302us/step - loss: 0.6593 - accuracy: 0.6080 - val_loss: 0.7103 - val_accuracy: 0.5250\n",
      "Epoch 10/50\n",
      "880/880 [==============================] - 0s 292us/step - loss: 0.6531 - accuracy: 0.6420 - val_loss: 0.7419 - val_accuracy: 0.5250\n",
      "Epoch 11/50\n",
      "880/880 [==============================] - 0s 296us/step - loss: 0.6491 - accuracy: 0.6455 - val_loss: 0.7407 - val_accuracy: 0.5250\n",
      "Epoch 12/50\n",
      "880/880 [==============================] - 0s 295us/step - loss: 0.6411 - accuracy: 0.6466 - val_loss: 0.7432 - val_accuracy: 0.4875\n",
      "Epoch 13/50\n",
      "880/880 [==============================] - 0s 300us/step - loss: 0.6331 - accuracy: 0.6602 - val_loss: 0.7648 - val_accuracy: 0.4875\n",
      "Epoch 14/50\n",
      "880/880 [==============================] - 0s 294us/step - loss: 0.6271 - accuracy: 0.6511 - val_loss: 0.7632 - val_accuracy: 0.4625\n",
      "Epoch 15/50\n",
      "880/880 [==============================] - 0s 294us/step - loss: 0.6236 - accuracy: 0.6307 - val_loss: 0.7967 - val_accuracy: 0.5000\n",
      "Epoch 16/50\n",
      "880/880 [==============================] - 0s 298us/step - loss: 0.6153 - accuracy: 0.6500 - val_loss: 0.8134 - val_accuracy: 0.4875\n",
      "Epoch 17/50\n",
      "880/880 [==============================] - 0s 301us/step - loss: 0.5970 - accuracy: 0.6909 - val_loss: 0.8189 - val_accuracy: 0.4750\n",
      "Epoch 18/50\n",
      "880/880 [==============================] - 0s 300us/step - loss: 0.5981 - accuracy: 0.6739 - val_loss: 0.8146 - val_accuracy: 0.4375\n",
      "Epoch 19/50\n",
      "880/880 [==============================] - 0s 292us/step - loss: 0.5794 - accuracy: 0.6966 - val_loss: 0.8441 - val_accuracy: 0.4750\n",
      "Epoch 20/50\n",
      "880/880 [==============================] - 0s 301us/step - loss: 0.5757 - accuracy: 0.6932 - val_loss: 0.9281 - val_accuracy: 0.5000\n",
      "Epoch 21/50\n",
      "880/880 [==============================] - 0s 302us/step - loss: 0.5684 - accuracy: 0.6943 - val_loss: 0.8822 - val_accuracy: 0.4375\n",
      "Epoch 22/50\n",
      "880/880 [==============================] - 0s 300us/step - loss: 0.5539 - accuracy: 0.7045 - val_loss: 0.9095 - val_accuracy: 0.4875\n",
      "Epoch 23/50\n",
      "880/880 [==============================] - 0s 308us/step - loss: 0.5486 - accuracy: 0.7034 - val_loss: 0.9412 - val_accuracy: 0.4625\n",
      "Epoch 24/50\n",
      "880/880 [==============================] - 0s 293us/step - loss: 0.5463 - accuracy: 0.7045 - val_loss: 0.9243 - val_accuracy: 0.4625\n",
      "Epoch 25/50\n",
      "880/880 [==============================] - 0s 301us/step - loss: 0.5346 - accuracy: 0.7159 - val_loss: 0.9627 - val_accuracy: 0.5000\n",
      "Epoch 26/50\n",
      "880/880 [==============================] - 0s 299us/step - loss: 0.5427 - accuracy: 0.6966 - val_loss: 0.9750 - val_accuracy: 0.4875\n",
      "Epoch 27/50\n",
      "880/880 [==============================] - 0s 299us/step - loss: 0.5235 - accuracy: 0.7068 - val_loss: 0.9488 - val_accuracy: 0.5000\n",
      "Epoch 28/50\n",
      "880/880 [==============================] - 0s 292us/step - loss: 0.5164 - accuracy: 0.7125 - val_loss: 0.9944 - val_accuracy: 0.4625\n",
      "Epoch 29/50\n",
      "880/880 [==============================] - 0s 300us/step - loss: 0.5212 - accuracy: 0.7193 - val_loss: 1.0160 - val_accuracy: 0.4375\n",
      "Epoch 30/50\n",
      "880/880 [==============================] - 0s 299us/step - loss: 0.5170 - accuracy: 0.7102 - val_loss: 1.0168 - val_accuracy: 0.5000\n",
      "Epoch 31/50\n",
      "880/880 [==============================] - 0s 292us/step - loss: 0.5091 - accuracy: 0.7227 - val_loss: 1.0177 - val_accuracy: 0.4750\n",
      "Epoch 32/50\n",
      "880/880 [==============================] - 0s 292us/step - loss: 0.5077 - accuracy: 0.7148 - val_loss: 1.0504 - val_accuracy: 0.5000\n",
      "Epoch 33/50\n",
      "880/880 [==============================] - 0s 294us/step - loss: 0.5142 - accuracy: 0.7114 - val_loss: 1.1168 - val_accuracy: 0.4875\n",
      "Epoch 34/50\n",
      "880/880 [==============================] - 0s 290us/step - loss: 0.5012 - accuracy: 0.7295 - val_loss: 1.0713 - val_accuracy: 0.5000\n",
      "Epoch 35/50\n",
      "880/880 [==============================] - 0s 300us/step - loss: 0.5004 - accuracy: 0.7102 - val_loss: 1.0852 - val_accuracy: 0.5000\n",
      "Epoch 36/50\n",
      "880/880 [==============================] - 0s 295us/step - loss: 0.4928 - accuracy: 0.7307 - val_loss: 1.1167 - val_accuracy: 0.4875\n",
      "Epoch 37/50\n",
      "880/880 [==============================] - 0s 276us/step - loss: 0.4974 - accuracy: 0.7273 - val_loss: 1.1017 - val_accuracy: 0.5000\n",
      "Epoch 38/50\n",
      "880/880 [==============================] - 0s 264us/step - loss: 0.4930 - accuracy: 0.7091 - val_loss: 1.1513 - val_accuracy: 0.5250\n",
      "Epoch 39/50\n",
      "880/880 [==============================] - 0s 266us/step - loss: 0.4852 - accuracy: 0.7318 - val_loss: 1.1311 - val_accuracy: 0.4875\n",
      "Epoch 40/50\n",
      "880/880 [==============================] - 0s 265us/step - loss: 0.4934 - accuracy: 0.7261 - val_loss: 1.1148 - val_accuracy: 0.4500\n",
      "Epoch 41/50\n",
      "880/880 [==============================] - 0s 265us/step - loss: 0.4931 - accuracy: 0.7284 - val_loss: 1.1025 - val_accuracy: 0.5000\n",
      "Epoch 42/50\n",
      "880/880 [==============================] - 0s 267us/step - loss: 0.4868 - accuracy: 0.7284 - val_loss: 1.1378 - val_accuracy: 0.4750\n",
      "Epoch 43/50\n",
      "880/880 [==============================] - 0s 265us/step - loss: 0.4850 - accuracy: 0.7216 - val_loss: 1.1945 - val_accuracy: 0.5125\n",
      "Epoch 44/50\n",
      "880/880 [==============================] - 0s 247us/step - loss: 0.4766 - accuracy: 0.7364 - val_loss: 1.1587 - val_accuracy: 0.4500\n",
      "Epoch 45/50\n",
      "880/880 [==============================] - 0s 217us/step - loss: 0.4778 - accuracy: 0.7364 - val_loss: 1.2073 - val_accuracy: 0.4625\n",
      "Epoch 46/50\n",
      "880/880 [==============================] - 0s 213us/step - loss: 0.4751 - accuracy: 0.7295 - val_loss: 1.2027 - val_accuracy: 0.5625\n",
      "Epoch 47/50\n",
      "880/880 [==============================] - 0s 212us/step - loss: 0.4822 - accuracy: 0.7318 - val_loss: 1.1435 - val_accuracy: 0.4750\n",
      "Epoch 48/50\n",
      "880/880 [==============================] - 0s 212us/step - loss: 0.4778 - accuracy: 0.7398 - val_loss: 1.1666 - val_accuracy: 0.4750\n",
      "Epoch 49/50\n",
      "880/880 [==============================] - 0s 211us/step - loss: 0.4747 - accuracy: 0.7273 - val_loss: 1.2019 - val_accuracy: 0.5125\n",
      "Epoch 50/50\n",
      "880/880 [==============================] - 0s 215us/step - loss: 0.4832 - accuracy: 0.7216 - val_loss: 1.2085 - val_accuracy: 0.5125\n",
      "Score for fold 9: loss of 1.208544945716858; accuracy of 51.249998807907104%\n",
      "Model: \"sequential_142\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_706 (Dense)            (None, 100)               2500      \n",
      "_________________________________________________________________\n",
      "dense_707 (Dense)            (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "dense_708 (Dense)            (None, 25)                1275      \n",
      "_________________________________________________________________\n",
      "dense_709 (Dense)            (None, 10)                260       \n",
      "_________________________________________________________________\n",
      "dense_710 (Dense)            (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 9,096\n",
      "Trainable params: 9,096\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 10 ...\n",
      "Train on 880 samples, validate on 80 samples\n",
      "Epoch 1/50\n",
      "880/880 [==============================] - 3s 4ms/step - loss: 0.6946 - accuracy: 0.5011 - val_loss: 0.6915 - val_accuracy: 0.5000\n",
      "Epoch 2/50\n",
      "880/880 [==============================] - 0s 298us/step - loss: 0.6909 - accuracy: 0.5375 - val_loss: 0.6926 - val_accuracy: 0.5250\n",
      "Epoch 3/50\n",
      "880/880 [==============================] - 0s 304us/step - loss: 0.6896 - accuracy: 0.5364 - val_loss: 0.6939 - val_accuracy: 0.5125\n",
      "Epoch 4/50\n",
      "880/880 [==============================] - 0s 301us/step - loss: 0.6882 - accuracy: 0.5409 - val_loss: 0.6948 - val_accuracy: 0.5250\n",
      "Epoch 5/50\n",
      "880/880 [==============================] - 0s 301us/step - loss: 0.6854 - accuracy: 0.5534 - val_loss: 0.6952 - val_accuracy: 0.5250\n",
      "Epoch 6/50\n",
      "880/880 [==============================] - 0s 301us/step - loss: 0.6831 - accuracy: 0.5682 - val_loss: 0.6984 - val_accuracy: 0.5250\n",
      "Epoch 7/50\n",
      "880/880 [==============================] - 0s 302us/step - loss: 0.6811 - accuracy: 0.5602 - val_loss: 0.6973 - val_accuracy: 0.5500\n",
      "Epoch 8/50\n",
      "880/880 [==============================] - 0s 301us/step - loss: 0.6783 - accuracy: 0.5761 - val_loss: 0.7020 - val_accuracy: 0.4500\n",
      "Epoch 9/50\n",
      "880/880 [==============================] - 0s 303us/step - loss: 0.6786 - accuracy: 0.6023 - val_loss: 0.7079 - val_accuracy: 0.5375\n",
      "Epoch 10/50\n",
      "880/880 [==============================] - 0s 300us/step - loss: 0.6718 - accuracy: 0.5920 - val_loss: 0.7069 - val_accuracy: 0.4875\n",
      "Epoch 11/50\n",
      "880/880 [==============================] - 0s 301us/step - loss: 0.6685 - accuracy: 0.5989 - val_loss: 0.7174 - val_accuracy: 0.5375\n",
      "Epoch 12/50\n",
      "880/880 [==============================] - 0s 293us/step - loss: 0.6634 - accuracy: 0.6148 - val_loss: 0.7174 - val_accuracy: 0.5125\n",
      "Epoch 13/50\n",
      "880/880 [==============================] - 0s 299us/step - loss: 0.6566 - accuracy: 0.6216 - val_loss: 0.7216 - val_accuracy: 0.5000\n",
      "Epoch 14/50\n",
      "880/880 [==============================] - 0s 292us/step - loss: 0.6520 - accuracy: 0.6284 - val_loss: 0.7148 - val_accuracy: 0.5125\n",
      "Epoch 15/50\n",
      "880/880 [==============================] - 0s 299us/step - loss: 0.6506 - accuracy: 0.6273 - val_loss: 0.7305 - val_accuracy: 0.5500\n",
      "Epoch 16/50\n",
      "880/880 [==============================] - 0s 299us/step - loss: 0.6388 - accuracy: 0.6398 - val_loss: 0.7355 - val_accuracy: 0.5125\n",
      "Epoch 17/50\n",
      "880/880 [==============================] - 0s 299us/step - loss: 0.6326 - accuracy: 0.6500 - val_loss: 0.7394 - val_accuracy: 0.4875\n",
      "Epoch 18/50\n",
      "880/880 [==============================] - 0s 301us/step - loss: 0.6243 - accuracy: 0.6534 - val_loss: 0.7500 - val_accuracy: 0.5375\n",
      "Epoch 19/50\n",
      "880/880 [==============================] - 0s 292us/step - loss: 0.6209 - accuracy: 0.6568 - val_loss: 0.7507 - val_accuracy: 0.5125\n",
      "Epoch 20/50\n",
      "880/880 [==============================] - 0s 295us/step - loss: 0.6056 - accuracy: 0.6693 - val_loss: 0.7546 - val_accuracy: 0.5250\n",
      "Epoch 21/50\n",
      "880/880 [==============================] - 0s 300us/step - loss: 0.6024 - accuracy: 0.6648 - val_loss: 0.7679 - val_accuracy: 0.5125\n",
      "Epoch 22/50\n",
      "880/880 [==============================] - 0s 303us/step - loss: 0.5925 - accuracy: 0.6830 - val_loss: 0.7685 - val_accuracy: 0.5125\n",
      "Epoch 23/50\n",
      "880/880 [==============================] - 0s 302us/step - loss: 0.5811 - accuracy: 0.6920 - val_loss: 0.7863 - val_accuracy: 0.5250\n",
      "Epoch 24/50\n",
      "880/880 [==============================] - 0s 301us/step - loss: 0.5776 - accuracy: 0.6705 - val_loss: 0.8132 - val_accuracy: 0.5375\n",
      "Epoch 25/50\n",
      "880/880 [==============================] - 0s 300us/step - loss: 0.5655 - accuracy: 0.6989 - val_loss: 0.8712 - val_accuracy: 0.4875\n",
      "Epoch 26/50\n",
      "880/880 [==============================] - 0s 293us/step - loss: 0.5689 - accuracy: 0.6955 - val_loss: 0.8207 - val_accuracy: 0.5000\n",
      "Epoch 27/50\n",
      "880/880 [==============================] - 0s 294us/step - loss: 0.5531 - accuracy: 0.6898 - val_loss: 0.8440 - val_accuracy: 0.4625\n",
      "Epoch 28/50\n",
      "880/880 [==============================] - 0s 292us/step - loss: 0.5465 - accuracy: 0.7057 - val_loss: 0.8405 - val_accuracy: 0.5125\n",
      "Epoch 29/50\n",
      "880/880 [==============================] - 0s 292us/step - loss: 0.5421 - accuracy: 0.7125 - val_loss: 0.8697 - val_accuracy: 0.5250\n",
      "Epoch 30/50\n",
      "880/880 [==============================] - 0s 299us/step - loss: 0.5279 - accuracy: 0.7136 - val_loss: 0.9001 - val_accuracy: 0.5250\n",
      "Epoch 31/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "880/880 [==============================] - 0s 299us/step - loss: 0.5332 - accuracy: 0.6977 - val_loss: 0.8804 - val_accuracy: 0.4625\n",
      "Epoch 32/50\n",
      "880/880 [==============================] - 0s 294us/step - loss: 0.5243 - accuracy: 0.7239 - val_loss: 0.8882 - val_accuracy: 0.5250\n",
      "Epoch 33/50\n",
      "880/880 [==============================] - 0s 290us/step - loss: 0.5123 - accuracy: 0.7273 - val_loss: 0.9018 - val_accuracy: 0.5250\n",
      "Epoch 34/50\n",
      "880/880 [==============================] - 0s 295us/step - loss: 0.5122 - accuracy: 0.7148 - val_loss: 0.8895 - val_accuracy: 0.5000\n",
      "Epoch 35/50\n",
      "880/880 [==============================] - 0s 291us/step - loss: 0.5183 - accuracy: 0.7136 - val_loss: 0.9279 - val_accuracy: 0.5125\n",
      "Epoch 36/50\n",
      "880/880 [==============================] - 0s 300us/step - loss: 0.5044 - accuracy: 0.7205 - val_loss: 0.9156 - val_accuracy: 0.5250\n",
      "Epoch 37/50\n",
      "880/880 [==============================] - 0s 300us/step - loss: 0.5011 - accuracy: 0.7341 - val_loss: 0.9409 - val_accuracy: 0.5375\n",
      "Epoch 38/50\n",
      "880/880 [==============================] - 0s 301us/step - loss: 0.5091 - accuracy: 0.7284 - val_loss: 0.9385 - val_accuracy: 0.5000\n",
      "Epoch 39/50\n",
      "880/880 [==============================] - 0s 298us/step - loss: 0.4987 - accuracy: 0.7250 - val_loss: 0.9630 - val_accuracy: 0.5250\n",
      "Epoch 40/50\n",
      "880/880 [==============================] - 0s 294us/step - loss: 0.4967 - accuracy: 0.7341 - val_loss: 1.0535 - val_accuracy: 0.5125\n",
      "Epoch 41/50\n",
      "880/880 [==============================] - 0s 300us/step - loss: 0.4985 - accuracy: 0.7250 - val_loss: 1.0053 - val_accuracy: 0.5000\n",
      "Epoch 42/50\n",
      "880/880 [==============================] - 0s 294us/step - loss: 0.4972 - accuracy: 0.7182 - val_loss: 1.0285 - val_accuracy: 0.4750\n",
      "Epoch 43/50\n",
      "880/880 [==============================] - 0s 293us/step - loss: 0.5172 - accuracy: 0.6977 - val_loss: 1.0131 - val_accuracy: 0.4875\n",
      "Epoch 44/50\n",
      "880/880 [==============================] - 0s 293us/step - loss: 0.5057 - accuracy: 0.7216 - val_loss: 0.9612 - val_accuracy: 0.5250\n",
      "Epoch 45/50\n",
      "880/880 [==============================] - 0s 292us/step - loss: 0.4889 - accuracy: 0.7250 - val_loss: 1.0061 - val_accuracy: 0.4875\n",
      "Epoch 46/50\n",
      "880/880 [==============================] - 0s 300us/step - loss: 0.4930 - accuracy: 0.7227 - val_loss: 1.0123 - val_accuracy: 0.5375\n",
      "Epoch 47/50\n",
      "880/880 [==============================] - 0s 292us/step - loss: 0.4954 - accuracy: 0.7159 - val_loss: 1.0383 - val_accuracy: 0.5375\n",
      "Epoch 48/50\n",
      "880/880 [==============================] - 0s 299us/step - loss: 0.4789 - accuracy: 0.7295 - val_loss: 1.0834 - val_accuracy: 0.5250\n",
      "Epoch 49/50\n",
      "880/880 [==============================] - 0s 295us/step - loss: 0.4864 - accuracy: 0.7125 - val_loss: 1.0243 - val_accuracy: 0.5500\n",
      "Epoch 50/50\n",
      "880/880 [==============================] - 0s 296us/step - loss: 0.4723 - accuracy: 0.7443 - val_loss: 1.1305 - val_accuracy: 0.5250\n",
      "Score for fold 10: loss of 1.1304893970489502; accuracy of 52.49999761581421%\n"
     ]
    }
   ],
   "source": [
    "# Define per-fold score containers <-- these are new\n",
    "acc_per_fold = []\n",
    "loss_per_fold = []\n",
    "epochs = 50\n",
    "fold_no = 1\n",
    "for train, test in kfold.split(X_train, y_train):\n",
    "\n",
    "  # Define the model architecture\n",
    "  model = Sequential()\n",
    "  model.add(Dense(units = 100 , activation = 'relu' , input_dim = X_train.shape[1]))\n",
    "  model.add(Dense(units = 50 , activation = 'relu'))\n",
    "  model.add(Dense(units = 25 , activation = 'relu'))\n",
    "  model.add(Dense(units = 10 , activation = 'relu'))\n",
    "  #model.add(Dense(units = 2 , activation = 'softmax'))\n",
    "  model.add(Dense(units = 1 , activation = 'sigmoid'))\n",
    "  #loss_fn = keras.losses.SparseCategoricalCrossentropy()\n",
    "  loss_fn = tf.keras.losses.BinaryCrossentropy(from_logits=False, label_smoothing=0, reduction=\"auto\", name=\"binary_crossentropy\")\n",
    "  model.compile(optimizer = 'adam' , loss = loss_fn , metrics = ['accuracy'])\n",
    "  model.summary()\n",
    "\n",
    "  # Generate a print\n",
    "  print('------------------------------------------------------------------------')\n",
    "  print(f'Training for fold {fold_no} ...')\n",
    "  # Fit data to model\n",
    "  history = model.fit(X_train,y_train, validation_data=(X_test, y_test), epochs = epochs)\n",
    "\n",
    "  # Generate generalization metrics\n",
    "  scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "  print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n",
    "  acc_per_fold.append(scores[1] * 100)\n",
    "  loss_per_fold.append(scores[0])\n",
    "\n",
    "  # Increase fold number\n",
    "  fold_no = fold_no + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save raw results Confusion Matrix\n",
    "conf_scores_csv = pd.DataFrame({\"acc_per_fold\" : acc_per_fold, \"loss_per_fold\" : loss_per_fold})\n",
    "conf_scores_csv.to_csv(\"Sensor_data_classification_demographic_comorbidity_Neural_Network_box_plot.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80/80 [==============================] - 0s 318us/step\n",
      "Test loss: 1.1304893970489502\n",
      "Test accuracy: 0.5249999761581421\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(X_test, y_test, verbose=1)\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(X_test)     \n",
    "#predictions = np.argmax(predictions, axis = 1) # We take the highest probability\n",
    "#predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Random Forest and Logistoc Regression\n",
    "#Pred_score = model.predict_proba(X_test)\n",
    "#Pred_score = Pred_score.ravel().tolist()\n",
    "#len(Pred_score)\n",
    "#res = pd.DataFrame(Pred_score)\n",
    "#res.index = X_test.index # its important for comparison\n",
    "#res.columns = [\"Sepsis\",\"Non-Sepsis\"]\n",
    "#res.to_csv(\"sxore_xx.csv\")\n",
    "#res.to_csv(\"lateFusion_results_4hr_36000samples_1Dec.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "#res = pd.DataFrame(y_pred)\n",
    "#res.index = X_test.index # its important for comparison\n",
    "#res.columns = [\"Sepsis\",\"Non-Sepsis\"]\n",
    "#res.to_csv(\"prediction_Demo_Reason_results_19Jan2021.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "#history_dict = history.history\n",
    "#print(history_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.arange(0, epochs), history.history[\"loss\"], label=\"train_loss\")\n",
    "plt.plot(np.arange(0, epochs), history.history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.plot(np.arange(0, epochs), history.history[\"accuracy\"], label=\"train_acc\")\n",
    "plt.plot(np.arange(0, epochs), history.history[\"val_accuracy\"], label=\"val_acc\")\n",
    "plt.title(\"Training Loss and Accuracy curves\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss/Accuracy\")\n",
    "plt.legend(loc=\"lower left\")\n",
    "#plt.savefig(args[\"plot\"])\n",
    "plt.show()\n",
    "#plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "    Sepsis(0)       0.62      0.52      0.57        48\n",
      "Non-Sepsis(1)       0.42      0.53      0.47        32\n",
      "\n",
      "     accuracy                           0.53        80\n",
      "    macro avg       0.53      0.53      0.52        80\n",
      " weighted avg       0.55      0.53      0.53        80\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "y_pred = model.predict(X_test)\n",
    "# Generate a classification report\n",
    "report = classification_report(y_pred.round(), y_test, target_names=['Sepsis(0)','Non-Sepsis(1)'])\n",
    "#report = classification_report(y_pred, y_test, target_names=['Sepsis(0)','Non-Sepsis(1)'])\n",
    "\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "# Predict the values from the validation dataset\n",
    "\n",
    "# compute the confusion matrix\n",
    "confusion_mtx = confusion_matrix(y_pred.round(), y_test) \n",
    "# plot the confusion matrix\n",
    "f,ax = plt.subplots(figsize=(8, 8))\n",
    "sns.heatmap(confusion_mtx, annot=True, linewidths=0.01,cmap=\"Greens\",linecolor=\"gray\", fmt= '.1f',ax=ax)\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = y_pred.ravel().tolist()\n",
    "# Save raw results Confusion Matrix\n",
    "conf_scores_csv = pd.DataFrame({\"y_test\" : y_test, \"y_pred\" : y_pred})\n",
    "conf_scores_csv.to_csv(\"Sensor_data_classification_demographic_comorbidity_Neural_Network_confusion_matrix.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
